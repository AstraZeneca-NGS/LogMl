{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LogMl Log(ML) is a framework that helps automate many steps in machine learning projects and let you quickly generate baseline results. Why? There is a considerable amount is setup, boiler-plate code, analysis in every ML/AI project. Log(ML) performs most of these boring tasks, so you can focus on what's important and adds value. Log(ML) performs a consistent data science pipeline, keeping track every action and saving all results and models automatically. Log(ML) Goals: What does Log(ML) do for me? Log(ML) is designed to: - Enforce best practices - Perform a set of common, well defined, well tested analyses - Quickly turn around the first analysis results - Facilitates logging in ML projects: No more writing down results in a notepad, Log(ML) creates log file in a systematic manner - Save models and results: Log(ML) saves all your models, so you can always retrieve the best ones. Architecture: How does Log(ML) work? Log(ML) has a standard \"data science workflow\" (a.k.a. pipeline). The workflow include several steps, such as data transformation, data augmentation, data exploration, feature importance, model training, hyper-parameter search, cross-validation, etc. Each step in the workflow can be customized either in a configuration YAML file or adding custom Python code.","title":"Home"},{"location":"#logml","text":"Log(ML) is a framework that helps automate many steps in machine learning projects and let you quickly generate baseline results. Why? There is a considerable amount is setup, boiler-plate code, analysis in every ML/AI project. Log(ML) performs most of these boring tasks, so you can focus on what's important and adds value. Log(ML) performs a consistent data science pipeline, keeping track every action and saving all results and models automatically. Log(ML) Goals: What does Log(ML) do for me? Log(ML) is designed to: - Enforce best practices - Perform a set of common, well defined, well tested analyses - Quickly turn around the first analysis results - Facilitates logging in ML projects: No more writing down results in a notepad, Log(ML) creates log file in a systematic manner - Save models and results: Log(ML) saves all your models, so you can always retrieve the best ones. Architecture: How does Log(ML) work? Log(ML) has a standard \"data science workflow\" (a.k.a. pipeline). The workflow include several steps, such as data transformation, data augmentation, data exploration, feature importance, model training, hyper-parameter search, cross-validation, etc. Each step in the workflow can be customized either in a configuration YAML file or adding custom Python code.","title":"LogMl"},{"location":"about/","text":"About This project is maintained by Pablo Cingolani and supported by AstraZeneca Bug reports Please send any bug reports using GitHub IMPORTANT: In order to reproduce the error condition, you MUST send a minimal example. The example should be: minimal, self contained, and only involve LogMl This means: Minimal: There should be only a few lines of code, a small dataset and configuration YAML specifically showing the problem (please do not send hundreds of lines of code, Gigabytes of data or your complete ML analysis). Self contained: No additional data should be required to run your example (e.g. I should not need you to send me additional 10TB data files to run the script). Only LogMl : No additional packages should be required to run your example (e.g. I should not need to install programs/packages on my computer to run your example).","title":"About"},{"location":"about/#about","text":"This project is maintained by Pablo Cingolani and supported by AstraZeneca","title":"About"},{"location":"about/#bug-reports","text":"Please send any bug reports using GitHub IMPORTANT: In order to reproduce the error condition, you MUST send a minimal example. The example should be: minimal, self contained, and only involve LogMl This means: Minimal: There should be only a few lines of code, a small dataset and configuration YAML specifically showing the problem (please do not send hundreds of lines of code, Gigabytes of data or your complete ML analysis). Self contained: No additional data should be required to run your example (e.g. I should not need you to send me additional 10TB data files to run the script). Only LogMl : No additional packages should be required to run your example (e.g. I should not need to install programs/packages on my computer to run your example).","title":"Bug reports"},{"location":"cross_validation/","text":"Cross-validation Cross-Validation can be used when training a model and calculating feature importance. The YAML configuration is quite simple, you need to enable cross-validation and then specify the cross-validation type and the parameters: The cross-validation workflow is implemented using SciKit learn's cross validation, on the methods and parameters see SciKit's documentation cross_validation: enable: True # Set this to 'True' to enable cross validation # Select one of the following algorithms and set the parameters KFold: n_splits: 5 # RepeatedKFold: # n_splits: 5 # n_repeats: 2 # LeaveOneOut: # LeavePOut: # p: 2 # ShuffleSplit: # n_splits: 5 # test_size: 0.25","title":"Cross validation"},{"location":"cross_validation/#cross-validation","text":"Cross-Validation can be used when training a model and calculating feature importance. The YAML configuration is quite simple, you need to enable cross-validation and then specify the cross-validation type and the parameters: The cross-validation workflow is implemented using SciKit learn's cross validation, on the methods and parameters see SciKit's documentation cross_validation: enable: True # Set this to 'True' to enable cross validation # Select one of the following algorithms and set the parameters KFold: n_splits: 5 # RepeatedKFold: # n_splits: 5 # n_repeats: 2 # LeaveOneOut: # LeavePOut: # p: 2 # ShuffleSplit: # n_splits: 5 # test_size: 0.25","title":"Cross-validation"},{"location":"datasets/","text":"Datasets The first steps in a LogMl_workflow perform dataset operations. There are several named_workflow_steps taking care of different aspects of dataset processing to make sure is suitable for training a model dataset_load : Load a dataset from a file dataset_create : Create a dataset using a user_defined_function dataset_preprocess : Pre-process data, e.g. to make it suitable for model training. dataset_augment : Data augmentation dataset_split : Split data into train / validate / test datasets dataset_inout : Obtain inputs and output for each (train / validate / test) dataset. dataset_save : Save datasets to a file Config_YAML There are several parameters that can be defined in the config_YAML : dataset: # Dataset type: 'df' means dataFrame dataset_type: 'df' # Dataset name: # A simple name for the dataset. # It is appended to dataset_path to create dataset_file_name dataset_name: 'my_dataset' # Dataset Path: Path to use when loading and saving datasets dataset_path: 'data/my_dataset' # If set, loading dataset from pickle file will be skipped do_not_load_pickle: false # If set, saving dataset will be skipped do_not_save: false # Use all inputs, no outputs. # I.e. do not split in/out. # E.g. unsupervised learning is_use_all_inputs: false # Use default 'in_out' method is_use_default_in_out: true # Use default preprocess is_use_default_preprocess: true # Use (internal) 'split' function if none is provided by the user? # Note: This function returns dataset as a list is_use_default_split: false # Use default transform operation is_use_default_transform: true # Output variables, a.k.a. dependent variables (i.e. what we want to predict) outputs: [] Dataset: Load This step typical attempts to load data from files (e.g. load a \"data frame\" or a set of images). Attempt to load from pickle file ( {dataset_path}/{dataset_name}.pkl ). If the files exists, load the dataset. Invoke a user_defined_function decorated with @dataset_load . If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, so LogMl will attempt to create a dataset (next step) Parameters to the user_defined_function are defined in config_YAML file section functions , sub-section dataset_load The dataset is marked to be saved Dataset: Create This part creates a dataset by invoking the user_defined_function decorated by @dataset_create If the dataset has been loaded (i.e. \"Load dataset\" step was successful), skip this step Invoke a user_defined_function decorated with @dataset_create : If there is no user_defined_function or the section is disabled in the config file (i.e. enable=False ), this step hard_fails , i.e. since LogMl doesn't have a dataset to work with (load and create steps both failed) it will exit with an error. Parameters to the user_defined_function are defined in config_YAML file section functions , sub-section dataset_create The return value from the user_defined_function is used as a dataset The dataset is marked to be saved Dataset: Preprocess This step is used to pre-process data in order to make the dataset compatible with the inputs required by the model (e.g. normalize values) If this step has already been performed (i.e. a dataset loaded from a pickle file that has already been pre-processed), the step is skipped Invoke a user_defined_function decorated with @dataset_preprocess : If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, no error is produced. Parameters: The first parameter is the dataset. Other parameters are defined in config_YAML file section functions , sub-section dataset_preprocess The return value replaces the original dataset The dataset is marked to be saved Dataset: Augment This step invokes the user_defined_function @augment to perform dataset augmentation If this step has already been performed (i.e. a dataset loaded from a pickle file that has already been augmented), the step is skipped Invoke a user_defined_function decorated with @dataset_augment : If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, no error is produced. Parameters: The first parameter is the dataset. Other parameters are defined in config_YAML file section functions , sub-section dataset_augment The return value replaces the original dataset The dataset is marked to be saved Dataset: Split This step us used to split the dataset into \"train\", \"test\" and \"validation\" datasets. If this step has already been performed (i.e. a dataset loaded from a pickle file that has already been split), the step is skipped Invoke a user_defined_function decorated with @dataset_split : If there is no user_defined_function @dataset_split or the section is disabled in the config file (i.e. enable=False ), this step has failed (no error is produced) attempt to use a LogMl_default_function of dataset_split . Parameters: The first parameter is the dataset. Other parameters are defined in config_YAML file section functions , sub-section dataset_split If the function is invoked, the return value must be a tuple of three datasets: (dataset_train, dataset_test, dataset_validate) The return value from the function replaces the original dataset (specifically, each value replaces the train/test/validate datasets) The dataset is marked to be saved Attempt to use a LogMl_default_function of dataset_split If config_YAML parameter is_use_default_split is set to False , the split step failed (no error is produced) The default split implementation attempts to split the dataset in three parts, defined by config_YAML file parameters split_test and split_validate in section dataset_split . If these parameters are not defined in the config_YAML file, the split section failed (no error is produced) Dataset: Inputs / Outputs For each dataset splie (train, validation, and test): Invoke a user_defined_function decorated with @dataset_inout : If there is no user_defined_function @dataset_inout or the section is disabled in the config file (i.e. enable=False ), this step fails (no error is produced) and LogMl attempts to use a LogMl_default_function for dataset_inout . Parameters: The first parameter is the dataset, the second parameter is the datset name ( train , validate , or test ). Other parameters are defined in config_YAML file section functions , sub-section dataset_inout If the function is invoked, the return value must be a tuple: (inputs, outputs) The return value from the function replaces the original dataset (specifically, each value replaces the train/test/validate datasets) The dataset is marked to be saved Attempt to use a LogMl_default_function of dataset_inout If config_YAML parameter is_use_default_in_out is set to False , the split step hard_fails (execution is halted) The default implementation only returns the same dataset is the config_YAML parameter is_use_all_inputs is set. Otherwise it hard_fails . Dataset: Save If the dataset is marked to be saved in any of the previous steps, attempt to save the dataset. If the the YAML config variable do_not_save is set to True , this step is skipped If a user_defined_function decorated with @dataset_save exists, it is invoked Parameters: The first four parameters are dataset, dataset_train, dataset_test, dataset_validate . Other parameters are defined in config_YAML file section functions , sub-section dataset_save Otherwise the dataset is saved to the pickle file {dataset_path}/{dataset_name}.pkl DataFrames There are several useful LogMl_default_function then the dataset is a DataFrame, i.e. when config_YAML defines dataset_type='df' . Dataset (df): Default Load Load DataFrame from a CSV file using (using pandas.read_csv ). Dataset (df): Default Preprocess The default method for DataFrame pre-processing does: Sanitize variables names: Convert names so that characters outside the set [_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789] are converted to _ (underscore) Convert to categorical: Fields defined in the config_YAML file, section dataset , sub-section categories are converted into categorical data and converted to a numerical (integer) representation. Category 0 represents missing values. Convert to one-hot: Fields defined in the config_YAML file, section dataset , sub-section one_hot are converted into one-hot encoding. Also, any categorical field that has a cardinality (i.e. number of categories) equal or less then one_hot_max_cardinality is converted to one-hot encoding. If there are missing values, a column *_isna is added to the one-hot encoding. Missing data indicators: In any column having missing values that was not converted to date, categorical or one-hot; a new column *_na is created (where the value is '1' if the field has missing a value) and the missing values are replaced by the median of the non-missing values. Expand date/time features: Fields defined in the config_YAML file, section dataset , sub-section dates are treated as date/time when the dataFrame CSV is loaded and then expanded into several columns: [Year, Month, Day, DayOfWeek, Hour, Minute, Second, etc.] . Remove samples with missing outputs: Can be disables using config_YAML option remove_missing_outputs Drop low standard deviation fields: All fields having standard deviation equal or lower than std_threshold (by default 0.0 ) are dropped. Using the default value ( std_threshold=0.0 ) this means dropping all fields having the exact same value for all samples. Remove duplicated inputs (e.g. two columns that are exactly the same) Shuffle samples: Can be enabled using config_YAML option shuffle Balance dataset: re-sample categories having low number of samples. Can be enabled using config_YAML option balance Impute missing values. Can use several strategies: [mean, median, most_frequent, one, zero] Normalize variables: Can use several strategies: [standard, maxabs, minmax, minmax_neg, log, log1p, log_standard, log1p_standard, quantile] Config_YAML Options for datasets (df) pre-process dataset_preprocess: # Set to 'false' to disable this step enable: true # Categorical data # A list of categorical input variables # Can be followed by a list of categories to enforce some type of categorical order in the codes categories: UsageBand: ['Low', 'Medium', 'High'] ProductSize: ['Mini', 'Small', 'Compact', 'Medium', 'Large / Medium', 'Large'] # Categorical data: Match field names using regular expressions # A list of categorical input variables # Can be followed by a list of categories to enforce some type of categorical order in the codes categories_regex: - 'zzz_.*': ['low', 'mid', 'high'] - 'yyy_.*': True # List of data columns that are transformed into 'date/time' # These columns are also split into 'date parts' (year, month, day, day_of_week, etc.) dates: ['saledate'] # Sanitize column names: Convert column names so that characters outside the set [_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789] are converted to '_' (underscore) is_sanitize_column_names: True # One hot encoding: List of variable to transform to one_hot one_hot: ['Enclosure_Type'] # One hot encoding: Encode all variables having a cardinality less or equal to 'one_hot_max_cardinality' one_hot_max_cardinality: 7 # Remove repeated inputs (i.e. input variables that have the exact same values as other inputs) remove_equal_inputs: true # Remove rows having missing output/s remove_missing_outputs: true # Remove columns: List of columns to remove from dataFrame remove_columns: [] # Remove columns: List of columns to remove from dataFrame (after all transformations have been applied) remove_columns_after: [] # Shuffle samples (e.g. shuffle rows in dataframe) shuffle: false # Drop inputs having standard deviation below this limit std_threshold: 0.0001 # Balance an unbalanced datasets (classification models only, since outputs must be cathegorical) balance: false # Impute values # Imputation method name, followed by list of variables, regex or 'true' (this last option means \"use as default method\") # that all variables should be imputed using this method impute: # Use the mean value mean: ['x2', 'x5', 'xx.*'] # Impute using the median value median: true # Impute using the most frequent value most_frequent: ['x3', 'x4'] # Impute by assigning value '1' one: ['x9', 'o.*'] # Do not impute these variables skip: ['x7', 'x8'] # Impute by assigning value '0' zero: ['x10', 'z.*'] # Normalize values # Normalization methods followed by list of variables, regex or 'true' meaning # that all variables should be normalized using this method normalize: # Transform so that mean(x)=0 var(x)=1 # Set to 'true' means to use as default (i.e. use this normalization # for all variables not explicitly define elsewhere) standard: true # Normalize dividing by max(abs(x_i)) maxabs: ['x2', 'x5', 'xx.*'] # Use min/max normalizer, i.e. transform to interval [0,1] minmax: ['x3'] # Use min/max normalizer with negative values, i.e. transform to interval [-1,1] minmax_neg: ['x6', 'neg_.*'] # Apply log(x) log: ['x9'] # Apply log(x+1) log1p: ['x10', 'x11'] # Apply log(x), then use standard transform: (log(x) - mean(log(x))) / std(log(x)) log_standard: ['x9'] # Apply log(x+1), then use standard transform: (log(x+1) - mean(log(x+1))) / std(log(x+1)) log1p_standard: ['x10', 'x11'] # Quantile transformation: This method transforms the features to follow a uniform or a normal distribution quantile: ['q.*'] # Do not normalize these variables skip: ['x7', 'x8'] Dataset (df): Default Augment The LogMl_default_function for dataset augmentation focusses on adding more columns to the dataset (i.e. augment the input variables), as opposed to augmenting the number of samples. Add Principal components (PCA values): Can be customized to act on several groups of variables. Add Non-negative Matrix Factorization components (NMF values): Can be customized to act on several groups of variables. Config_YAML Options for datasets (df) augment: dataset_augment: # Set to 'false' to disable this step enable: true # Add principal componenets # After the 'pca', you can add several \"sub-sections\": # name: # Unique name, used to add PCA fields on each sample (name_1, name_2, ... name_num) # num: 2 # Number of PCA componenets to add # fields: ['x.*'] # List of regular expression to match field names (only fields matching this list will be used) pca: pca_dna: num: 2 fields: ['DNA_.*'] pca_log2: num: 3 fields: ['log2ratio_.*', '.*_LOG2F'] # Add NMF (non-negative matrix factorization) # After the 'nmf', you can add several \"sub-sections\": # name: # Unique name, used to add NMF fields on each sample (name_1, name_2, ... name_num) # num: 2 # Number of NMF componenets to add # fields: ['x.*'] # List of regular expression to match field names (only fields matching this list will be used) nmf: nmf_dna: num: 2 fields: ['DNA_.*'] nmf_expr: num: 3 fields: ['expr_.*', '.*_EXPR']","title":"Workflow: Datasets"},{"location":"datasets/#datasets","text":"The first steps in a LogMl_workflow perform dataset operations. There are several named_workflow_steps taking care of different aspects of dataset processing to make sure is suitable for training a model dataset_load : Load a dataset from a file dataset_create : Create a dataset using a user_defined_function dataset_preprocess : Pre-process data, e.g. to make it suitable for model training. dataset_augment : Data augmentation dataset_split : Split data into train / validate / test datasets dataset_inout : Obtain inputs and output for each (train / validate / test) dataset. dataset_save : Save datasets to a file","title":"Datasets"},{"location":"datasets/#config_yaml","text":"There are several parameters that can be defined in the config_YAML : dataset: # Dataset type: 'df' means dataFrame dataset_type: 'df' # Dataset name: # A simple name for the dataset. # It is appended to dataset_path to create dataset_file_name dataset_name: 'my_dataset' # Dataset Path: Path to use when loading and saving datasets dataset_path: 'data/my_dataset' # If set, loading dataset from pickle file will be skipped do_not_load_pickle: false # If set, saving dataset will be skipped do_not_save: false # Use all inputs, no outputs. # I.e. do not split in/out. # E.g. unsupervised learning is_use_all_inputs: false # Use default 'in_out' method is_use_default_in_out: true # Use default preprocess is_use_default_preprocess: true # Use (internal) 'split' function if none is provided by the user? # Note: This function returns dataset as a list is_use_default_split: false # Use default transform operation is_use_default_transform: true # Output variables, a.k.a. dependent variables (i.e. what we want to predict) outputs: []","title":"Config_YAML"},{"location":"datasets/#dataset-load","text":"This step typical attempts to load data from files (e.g. load a \"data frame\" or a set of images). Attempt to load from pickle file ( {dataset_path}/{dataset_name}.pkl ). If the files exists, load the dataset. Invoke a user_defined_function decorated with @dataset_load . If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, so LogMl will attempt to create a dataset (next step) Parameters to the user_defined_function are defined in config_YAML file section functions , sub-section dataset_load The dataset is marked to be saved","title":"Dataset: Load"},{"location":"datasets/#dataset-create","text":"This part creates a dataset by invoking the user_defined_function decorated by @dataset_create If the dataset has been loaded (i.e. \"Load dataset\" step was successful), skip this step Invoke a user_defined_function decorated with @dataset_create : If there is no user_defined_function or the section is disabled in the config file (i.e. enable=False ), this step hard_fails , i.e. since LogMl doesn't have a dataset to work with (load and create steps both failed) it will exit with an error. Parameters to the user_defined_function are defined in config_YAML file section functions , sub-section dataset_create The return value from the user_defined_function is used as a dataset The dataset is marked to be saved","title":"Dataset: Create"},{"location":"datasets/#dataset-preprocess","text":"This step is used to pre-process data in order to make the dataset compatible with the inputs required by the model (e.g. normalize values) If this step has already been performed (i.e. a dataset loaded from a pickle file that has already been pre-processed), the step is skipped Invoke a user_defined_function decorated with @dataset_preprocess : If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, no error is produced. Parameters: The first parameter is the dataset. Other parameters are defined in config_YAML file section functions , sub-section dataset_preprocess The return value replaces the original dataset The dataset is marked to be saved","title":"Dataset: Preprocess"},{"location":"datasets/#dataset-augment","text":"This step invokes the user_defined_function @augment to perform dataset augmentation If this step has already been performed (i.e. a dataset loaded from a pickle file that has already been augmented), the step is skipped Invoke a user_defined_function decorated with @dataset_augment : If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, no error is produced. Parameters: The first parameter is the dataset. Other parameters are defined in config_YAML file section functions , sub-section dataset_augment The return value replaces the original dataset The dataset is marked to be saved","title":"Dataset: Augment"},{"location":"datasets/#dataset-split","text":"This step us used to split the dataset into \"train\", \"test\" and \"validation\" datasets. If this step has already been performed (i.e. a dataset loaded from a pickle file that has already been split), the step is skipped Invoke a user_defined_function decorated with @dataset_split : If there is no user_defined_function @dataset_split or the section is disabled in the config file (i.e. enable=False ), this step has failed (no error is produced) attempt to use a LogMl_default_function of dataset_split . Parameters: The first parameter is the dataset. Other parameters are defined in config_YAML file section functions , sub-section dataset_split If the function is invoked, the return value must be a tuple of three datasets: (dataset_train, dataset_test, dataset_validate) The return value from the function replaces the original dataset (specifically, each value replaces the train/test/validate datasets) The dataset is marked to be saved Attempt to use a LogMl_default_function of dataset_split If config_YAML parameter is_use_default_split is set to False , the split step failed (no error is produced) The default split implementation attempts to split the dataset in three parts, defined by config_YAML file parameters split_test and split_validate in section dataset_split . If these parameters are not defined in the config_YAML file, the split section failed (no error is produced)","title":"Dataset: Split"},{"location":"datasets/#dataset-inputs-outputs","text":"For each dataset splie (train, validation, and test): Invoke a user_defined_function decorated with @dataset_inout : If there is no user_defined_function @dataset_inout or the section is disabled in the config file (i.e. enable=False ), this step fails (no error is produced) and LogMl attempts to use a LogMl_default_function for dataset_inout . Parameters: The first parameter is the dataset, the second parameter is the datset name ( train , validate , or test ). Other parameters are defined in config_YAML file section functions , sub-section dataset_inout If the function is invoked, the return value must be a tuple: (inputs, outputs) The return value from the function replaces the original dataset (specifically, each value replaces the train/test/validate datasets) The dataset is marked to be saved Attempt to use a LogMl_default_function of dataset_inout If config_YAML parameter is_use_default_in_out is set to False , the split step hard_fails (execution is halted) The default implementation only returns the same dataset is the config_YAML parameter is_use_all_inputs is set. Otherwise it hard_fails .","title":"Dataset: Inputs / Outputs"},{"location":"datasets/#dataset-save","text":"If the dataset is marked to be saved in any of the previous steps, attempt to save the dataset. If the the YAML config variable do_not_save is set to True , this step is skipped If a user_defined_function decorated with @dataset_save exists, it is invoked Parameters: The first four parameters are dataset, dataset_train, dataset_test, dataset_validate . Other parameters are defined in config_YAML file section functions , sub-section dataset_save Otherwise the dataset is saved to the pickle file {dataset_path}/{dataset_name}.pkl","title":"Dataset: Save"},{"location":"datasets/#dataframes","text":"There are several useful LogMl_default_function then the dataset is a DataFrame, i.e. when config_YAML defines dataset_type='df' .","title":"DataFrames"},{"location":"datasets/#dataset-df-default-load","text":"Load DataFrame from a CSV file using (using pandas.read_csv ).","title":"Dataset (df): Default Load"},{"location":"datasets/#dataset-df-default-preprocess","text":"The default method for DataFrame pre-processing does: Sanitize variables names: Convert names so that characters outside the set [_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789] are converted to _ (underscore) Convert to categorical: Fields defined in the config_YAML file, section dataset , sub-section categories are converted into categorical data and converted to a numerical (integer) representation. Category 0 represents missing values. Convert to one-hot: Fields defined in the config_YAML file, section dataset , sub-section one_hot are converted into one-hot encoding. Also, any categorical field that has a cardinality (i.e. number of categories) equal or less then one_hot_max_cardinality is converted to one-hot encoding. If there are missing values, a column *_isna is added to the one-hot encoding. Missing data indicators: In any column having missing values that was not converted to date, categorical or one-hot; a new column *_na is created (where the value is '1' if the field has missing a value) and the missing values are replaced by the median of the non-missing values. Expand date/time features: Fields defined in the config_YAML file, section dataset , sub-section dates are treated as date/time when the dataFrame CSV is loaded and then expanded into several columns: [Year, Month, Day, DayOfWeek, Hour, Minute, Second, etc.] . Remove samples with missing outputs: Can be disables using config_YAML option remove_missing_outputs Drop low standard deviation fields: All fields having standard deviation equal or lower than std_threshold (by default 0.0 ) are dropped. Using the default value ( std_threshold=0.0 ) this means dropping all fields having the exact same value for all samples. Remove duplicated inputs (e.g. two columns that are exactly the same) Shuffle samples: Can be enabled using config_YAML option shuffle Balance dataset: re-sample categories having low number of samples. Can be enabled using config_YAML option balance Impute missing values. Can use several strategies: [mean, median, most_frequent, one, zero] Normalize variables: Can use several strategies: [standard, maxabs, minmax, minmax_neg, log, log1p, log_standard, log1p_standard, quantile] Config_YAML Options for datasets (df) pre-process dataset_preprocess: # Set to 'false' to disable this step enable: true # Categorical data # A list of categorical input variables # Can be followed by a list of categories to enforce some type of categorical order in the codes categories: UsageBand: ['Low', 'Medium', 'High'] ProductSize: ['Mini', 'Small', 'Compact', 'Medium', 'Large / Medium', 'Large'] # Categorical data: Match field names using regular expressions # A list of categorical input variables # Can be followed by a list of categories to enforce some type of categorical order in the codes categories_regex: - 'zzz_.*': ['low', 'mid', 'high'] - 'yyy_.*': True # List of data columns that are transformed into 'date/time' # These columns are also split into 'date parts' (year, month, day, day_of_week, etc.) dates: ['saledate'] # Sanitize column names: Convert column names so that characters outside the set [_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789] are converted to '_' (underscore) is_sanitize_column_names: True # One hot encoding: List of variable to transform to one_hot one_hot: ['Enclosure_Type'] # One hot encoding: Encode all variables having a cardinality less or equal to 'one_hot_max_cardinality' one_hot_max_cardinality: 7 # Remove repeated inputs (i.e. input variables that have the exact same values as other inputs) remove_equal_inputs: true # Remove rows having missing output/s remove_missing_outputs: true # Remove columns: List of columns to remove from dataFrame remove_columns: [] # Remove columns: List of columns to remove from dataFrame (after all transformations have been applied) remove_columns_after: [] # Shuffle samples (e.g. shuffle rows in dataframe) shuffle: false # Drop inputs having standard deviation below this limit std_threshold: 0.0001 # Balance an unbalanced datasets (classification models only, since outputs must be cathegorical) balance: false # Impute values # Imputation method name, followed by list of variables, regex or 'true' (this last option means \"use as default method\") # that all variables should be imputed using this method impute: # Use the mean value mean: ['x2', 'x5', 'xx.*'] # Impute using the median value median: true # Impute using the most frequent value most_frequent: ['x3', 'x4'] # Impute by assigning value '1' one: ['x9', 'o.*'] # Do not impute these variables skip: ['x7', 'x8'] # Impute by assigning value '0' zero: ['x10', 'z.*'] # Normalize values # Normalization methods followed by list of variables, regex or 'true' meaning # that all variables should be normalized using this method normalize: # Transform so that mean(x)=0 var(x)=1 # Set to 'true' means to use as default (i.e. use this normalization # for all variables not explicitly define elsewhere) standard: true # Normalize dividing by max(abs(x_i)) maxabs: ['x2', 'x5', 'xx.*'] # Use min/max normalizer, i.e. transform to interval [0,1] minmax: ['x3'] # Use min/max normalizer with negative values, i.e. transform to interval [-1,1] minmax_neg: ['x6', 'neg_.*'] # Apply log(x) log: ['x9'] # Apply log(x+1) log1p: ['x10', 'x11'] # Apply log(x), then use standard transform: (log(x) - mean(log(x))) / std(log(x)) log_standard: ['x9'] # Apply log(x+1), then use standard transform: (log(x+1) - mean(log(x+1))) / std(log(x+1)) log1p_standard: ['x10', 'x11'] # Quantile transformation: This method transforms the features to follow a uniform or a normal distribution quantile: ['q.*'] # Do not normalize these variables skip: ['x7', 'x8']","title":"Dataset (df): Default Preprocess"},{"location":"datasets/#dataset-df-default-augment","text":"The LogMl_default_function for dataset augmentation focusses on adding more columns to the dataset (i.e. augment the input variables), as opposed to augmenting the number of samples. Add Principal components (PCA values): Can be customized to act on several groups of variables. Add Non-negative Matrix Factorization components (NMF values): Can be customized to act on several groups of variables. Config_YAML Options for datasets (df) augment: dataset_augment: # Set to 'false' to disable this step enable: true # Add principal componenets # After the 'pca', you can add several \"sub-sections\": # name: # Unique name, used to add PCA fields on each sample (name_1, name_2, ... name_num) # num: 2 # Number of PCA componenets to add # fields: ['x.*'] # List of regular expression to match field names (only fields matching this list will be used) pca: pca_dna: num: 2 fields: ['DNA_.*'] pca_log2: num: 3 fields: ['log2ratio_.*', '.*_LOG2F'] # Add NMF (non-negative matrix factorization) # After the 'nmf', you can add several \"sub-sections\": # name: # Unique name, used to add NMF fields on each sample (name_1, name_2, ... name_num) # num: 2 # Number of NMF componenets to add # fields: ['x.*'] # List of regular expression to match field names (only fields matching this list will be used) nmf: nmf_dna: num: 2 fields: ['DNA_.*'] nmf_expr: num: 3 fields: ['expr_.*', '.*_EXPR']","title":"Dataset (df): Default Augment"},{"location":"examples_custom_fucntions/","text":"Learning by examples This is Machine Learning, so let's learn by showing some examples...(hopefully you can generalize) In this section we introduce some examples on how to use Log(ML) and show how the framework simplifies some aspect fo machine learning projects. Basic setup Log(ML) can provide some default implementations for some steps of the workflow, but others you need to provide yourself (e.g. code to create your machine learning model). These steps are provided in the Python code you write. Both your Python code and the default Log(ML) implementations require parameters, these parameters are configured in a YAML file. So, a Log(ML) project consist of (at least) two parts: 1. A Python program 1. A YAML configuration file Example 1: A neural network for \"XOR\" In the code shown in example_01.py (see below) we train a neural network model to learn the \"XOR\" problem. We create three functions: - my_dataset_create : Create a dataset (a NumPy matrix) having the inputs and outputs for our problem. We create two columns (the inputs) of num_samples row or random numbers in the interval [-1, 1] . The third column (the output) is the \"XOR\" of the first two columns - my_model_create : Create a neural network using Tenforflow and Keras sequential mode. The network one hidden layer with num_neurons neurons - my_model_train : Train the neural network using a learning rate of learning_rate and epochs number of epochs. - my_model_eval : Evaluate the neural network. Note that the functions are decorated using Log(ML) decorators @dataset_create , @@model_create , @model_train , @model_evaluate Python code example_01.py : #!/usr/bin/env python3 import numpy as np import tensorflow as tf from logml import * from tensorflow.keras.layers import Dense from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adagrad @dataset_create def my_dataset_create(num_samples): x = 2 * np.random.rand(num_samples, 2) - 1 y = ((x[:, 0] > 0) ^ (x[:, 1] > 0)).astype('float').reshape(num_samples, 1) return np.concatenate((x, y), axis=1) @model_create def my_model_create(dataset, num_neurons): model = Sequential() model.add(Dense(num_neurons, activation='tanh', input_shape=(2,))) model.add(Dense(1, activation='tanh')) return model @model_train def my_model_train(model, dataset, learning_rate, epochs): model.compile(optimizer=Adagrad(lr=learning_rate), loss='mean_squared_error') return model.fit(dataset[:, 0:2], dataset[:, 2], epochs=epochs) @model_evaluate def my_model_eval(model, dataset): return model.evaluate(dataset[:, 0:2], dataset[:, 2]) ml = LogMl() ml() We also need to create a configuration YAML file (see below). This YAML file defines three sections: - dataset : Defines the name of the dataset and path to save dataset files. - train : Defines the name of the model and path to save model, model parameters and training results files. - functions : These define the values to pass to the functions defined in our python program (or Log(ML) default implementations). Configuration YAML file example_01.yaml dataset: dataset_name: 'example_01' dataset_path: 'data/example_01' model: model_name: 'example_01' model_path: 'data/example_01/model' functions: dataset_create: num_samples: 1000 dataset_split: split_test: 0.2 split_validate: 0.0 model_create: num_neurons: 3 model_train: epochs: 20 learning_rate: 0.3 A few remarks about the functions section: 1. The name of the parameters in the YAML must match exactly the name of the respective Python functions parameters 1. Python annotation matches the subsection in the YAML file (e.g. parameters defined YAML subsection dataset_create is called num_samples , which matches the parameter of the Python function annotated with @dataset_create ) 1. Since our @model_evaluate function doesn't take any additional arguments than the ones provided by Log(ML) (i.e. model and dataset ), we don't need to specify the sub-sections in our YAML file 1. The @dataset_split function was not implemented in our program, so Log(ML) will provide a default implementation. This default implementation uses the parameters split_test and split_validate (the dataset is split according to these numbers) Now we can run the program: # By default the expected config file name is \"ml.yaml\" so we provide an alternative name name with command line option \"-c\" $ ./example_01.py -c example_01.yaml Epoch 1/20 1000/1000 [==============================] - 0s 178us/sample - loss: 0.2416 Epoch 2/20 1000/1000 [==============================] - 0s 30us/sample - loss: 0.1588 ... Epoch 20/20 1000/1000 [==============================] - 0s 30us/sample - loss: 0.0949 So, Log(ML) performed a workflow that: 1. Invoked the function to create a dataset using the arguments from the YAML file (i.e. my_dataset_create(num_samples=20) ) 1. Invoked the function to create a model using as arguments the dataset plus the parameters from the YAML file (i.e. my_model_create(dataset, num_neurons=3) ) 1. Invoked the function to train the model using as arguments the model , the dataset plus the parameters from the YAML file (i.e. my_model_train(model, dataset, learning_rate=0.3, epochs=20) ) 1. Invoked the function to validate the model (evaluate on the validation dataset split) using only as arguments model , and dataset_validate (since there are no additional parameters from the YAML file) But Log(ML) it also did log a lot of information that is useful for future references. In this case, it saved the dataset to a pickle file ( example_01.pkl ), the all parameters used to create and train this model ( data/example_01/train/example_01.parameters.20190823.212609.830649.1.yaml ) and the full STDOUT/STDERR ( data/example_01/train/example_01.20190823.212609.830649.1.stdout and data/example_01/train/example_01.20190823.212609.830649.1.stderr ) $ ls data/example_01/* data/example_01/train/* data/example_01/example_01.pkl data/example_01/train/example_01.20190823.212609.830649.1.stdout data/example_01/train/example_01.20190823.212609.830649.1.stderr data/example_01/train/example_01.parameters.20190823.212609.830649.1.yaml Now we can change the parameters in the YAML file (for instance set learning_rate: 0.1 ) and run the program again. $ ./example_01.py -c example_01.yaml Epoch 1/20 1000/1000 [==============================] - 0s 184us/sample - loss: 0.2561 ... Epoch 20/20 1000/1000 [==============================] - 0s 23us/sample - loss: 0.1112 All the new log files will be created and we can keep track of our project and the parameters we used. OK, this model is not as good as the previous one, but fortunately we have all the logging information, so we don't have to remember the parameters we used for the best model. $ ls data/example_01/* data/example_01/train/* data/example_01/example_01.pkl data/example_01/train/example_01.20190823.213803.075040.1.stdout data/example_01/train/example_01.20190823.212609.830649.1.stderr data/example_01/train/example_01.parameters.20190823.212609.830649.1.yaml data/example_01/train/example_01.20190823.212609.830649.1.stdout data/example_01/train/example_01.parameters.20190823.213803.075040.1.yaml data/example_01/train/example_01.20190823.213803.075040.1.stderr Example 2: Hyper-parameter optimization Building on the previous example ( example_01.py and example_01.yaml ), let's assume that instead of trying to tune the learning_rate manually, we'd prefer to perform hyper-parameter optimization. In this example ( example_02 ), we'll set up hyper-parameter optimization on learning_rate . The python program remains exactly the same as in the previous example, we'll be adding a hyper-parameter optimization section to the YAML file. For the config YAML file (see example_02.yaml ), we jut add the following section: hyper_parameter_optimization: algorithm: 'tpe' max_evals: 100 space: model_train: learning_rate: ['uniform', 0.0, 0.5] We added a hyper_parameter_optimization section where we: - Define the hyper parameter algorithm ( tpe ) which is a Bayesian apprach - Set the number of evaluations to 100 - Define that we want to optimize the parameter learning_rate in the function @model_train using a uniform prior in the interval [0.0, 0.5] . We run the program: $ ./example_02.py -c example_02.yaml 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06<00:00, 1.44it/s, best loss: 0.07341234689950943] Here the hyper-parameter optimization is saying that the best loss found (with ten iterations) is 0.0734 . We also have all the parameter details, models, and STDOUT/STDERR for every single model created and trained: $ ls data/example_02/* data/example_02/train/* | cat data/example_02/example_02.pkl data/example_02/train/example_02.20190823.215947.132156.1.stderr data/example_02/train/example_02.20190823.215947.132156.1.stdout ... data/example_02/train/example_02.20190823.215953.151580.10.stderr data/example_02/train/example_02.20190823.215953.151580.10.stdout data/example_02/train/example_02.hyper_param_search.20190823.215953.151580.10.pkl data/example_02/train/example_02.parameters.20190823.215947.132156.1.yaml ... data/example_02/train/example_02.parameters.20190823.215953.151580.10.yaml Example 3: Neural network architecture optimization Now we build on the previous example (Example 2) by trying to optimize the neural network architecture. For this we just need to add a hyper parameter optimization when building the neural network (i.e. the @model_create step in the workflow). Simply add a line in the space definition within hyper_parameter_optimization section: The YAML is changed like this (see example_03.yaml ): hyper_parameter_optimization: ... space: model_create: num_neurons: ['randint', 5] ... Also we need a minor change in the python program is to ensure that we at least have one neuron in the hidden layer (otherwise the model doesn't make sense) So we add a single line to @model_create (see line num_neurons = max(num_neurons, 1) below): @model_create def my_model_create(dataset, num_neurons): model = Sequential() num_neurons = max(num_neurons, 1) # <-- Added this line model.add(Dense(num_neurons, activation='tanh', input_shape=(2,))) model.add(Dense(1, activation='tanh')) return model That's is, we have network architecture optimization ( num_neurons ) and hyper-parameter optimization ( learning_rate ). Let's run the program (output edited for readability): $ ./example_03.py -v -c example_03.yaml ... 2019-08-23 21:29:51,924 INFO Hyper parameter optimization: iteration: 10 ... best fit: 0.06886020198464393 best parameters: {'model_create': {'num_neurons': 3}, 'model_train': {'learning_rate': 0.22890998206259194}} ... The best parameters, for a 10 iteration hyper-optimization, are num_neurons=3 and learning_rate=0.2289 .","title":"Examples: Custom functions"},{"location":"examples_custom_fucntions/#learning-by-examples","text":"This is Machine Learning, so let's learn by showing some examples...(hopefully you can generalize) In this section we introduce some examples on how to use Log(ML) and show how the framework simplifies some aspect fo machine learning projects.","title":"Learning by examples"},{"location":"examples_custom_fucntions/#basic-setup","text":"Log(ML) can provide some default implementations for some steps of the workflow, but others you need to provide yourself (e.g. code to create your machine learning model). These steps are provided in the Python code you write. Both your Python code and the default Log(ML) implementations require parameters, these parameters are configured in a YAML file. So, a Log(ML) project consist of (at least) two parts: 1. A Python program 1. A YAML configuration file","title":"Basic setup"},{"location":"examples_custom_fucntions/#example-1-a-neural-network-for-xor","text":"In the code shown in example_01.py (see below) we train a neural network model to learn the \"XOR\" problem. We create three functions: - my_dataset_create : Create a dataset (a NumPy matrix) having the inputs and outputs for our problem. We create two columns (the inputs) of num_samples row or random numbers in the interval [-1, 1] . The third column (the output) is the \"XOR\" of the first two columns - my_model_create : Create a neural network using Tenforflow and Keras sequential mode. The network one hidden layer with num_neurons neurons - my_model_train : Train the neural network using a learning rate of learning_rate and epochs number of epochs. - my_model_eval : Evaluate the neural network. Note that the functions are decorated using Log(ML) decorators @dataset_create , @@model_create , @model_train , @model_evaluate Python code example_01.py : #!/usr/bin/env python3 import numpy as np import tensorflow as tf from logml import * from tensorflow.keras.layers import Dense from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adagrad @dataset_create def my_dataset_create(num_samples): x = 2 * np.random.rand(num_samples, 2) - 1 y = ((x[:, 0] > 0) ^ (x[:, 1] > 0)).astype('float').reshape(num_samples, 1) return np.concatenate((x, y), axis=1) @model_create def my_model_create(dataset, num_neurons): model = Sequential() model.add(Dense(num_neurons, activation='tanh', input_shape=(2,))) model.add(Dense(1, activation='tanh')) return model @model_train def my_model_train(model, dataset, learning_rate, epochs): model.compile(optimizer=Adagrad(lr=learning_rate), loss='mean_squared_error') return model.fit(dataset[:, 0:2], dataset[:, 2], epochs=epochs) @model_evaluate def my_model_eval(model, dataset): return model.evaluate(dataset[:, 0:2], dataset[:, 2]) ml = LogMl() ml() We also need to create a configuration YAML file (see below). This YAML file defines three sections: - dataset : Defines the name of the dataset and path to save dataset files. - train : Defines the name of the model and path to save model, model parameters and training results files. - functions : These define the values to pass to the functions defined in our python program (or Log(ML) default implementations). Configuration YAML file example_01.yaml dataset: dataset_name: 'example_01' dataset_path: 'data/example_01' model: model_name: 'example_01' model_path: 'data/example_01/model' functions: dataset_create: num_samples: 1000 dataset_split: split_test: 0.2 split_validate: 0.0 model_create: num_neurons: 3 model_train: epochs: 20 learning_rate: 0.3 A few remarks about the functions section: 1. The name of the parameters in the YAML must match exactly the name of the respective Python functions parameters 1. Python annotation matches the subsection in the YAML file (e.g. parameters defined YAML subsection dataset_create is called num_samples , which matches the parameter of the Python function annotated with @dataset_create ) 1. Since our @model_evaluate function doesn't take any additional arguments than the ones provided by Log(ML) (i.e. model and dataset ), we don't need to specify the sub-sections in our YAML file 1. The @dataset_split function was not implemented in our program, so Log(ML) will provide a default implementation. This default implementation uses the parameters split_test and split_validate (the dataset is split according to these numbers) Now we can run the program: # By default the expected config file name is \"ml.yaml\" so we provide an alternative name name with command line option \"-c\" $ ./example_01.py -c example_01.yaml Epoch 1/20 1000/1000 [==============================] - 0s 178us/sample - loss: 0.2416 Epoch 2/20 1000/1000 [==============================] - 0s 30us/sample - loss: 0.1588 ... Epoch 20/20 1000/1000 [==============================] - 0s 30us/sample - loss: 0.0949 So, Log(ML) performed a workflow that: 1. Invoked the function to create a dataset using the arguments from the YAML file (i.e. my_dataset_create(num_samples=20) ) 1. Invoked the function to create a model using as arguments the dataset plus the parameters from the YAML file (i.e. my_model_create(dataset, num_neurons=3) ) 1. Invoked the function to train the model using as arguments the model , the dataset plus the parameters from the YAML file (i.e. my_model_train(model, dataset, learning_rate=0.3, epochs=20) ) 1. Invoked the function to validate the model (evaluate on the validation dataset split) using only as arguments model , and dataset_validate (since there are no additional parameters from the YAML file) But Log(ML) it also did log a lot of information that is useful for future references. In this case, it saved the dataset to a pickle file ( example_01.pkl ), the all parameters used to create and train this model ( data/example_01/train/example_01.parameters.20190823.212609.830649.1.yaml ) and the full STDOUT/STDERR ( data/example_01/train/example_01.20190823.212609.830649.1.stdout and data/example_01/train/example_01.20190823.212609.830649.1.stderr ) $ ls data/example_01/* data/example_01/train/* data/example_01/example_01.pkl data/example_01/train/example_01.20190823.212609.830649.1.stdout data/example_01/train/example_01.20190823.212609.830649.1.stderr data/example_01/train/example_01.parameters.20190823.212609.830649.1.yaml Now we can change the parameters in the YAML file (for instance set learning_rate: 0.1 ) and run the program again. $ ./example_01.py -c example_01.yaml Epoch 1/20 1000/1000 [==============================] - 0s 184us/sample - loss: 0.2561 ... Epoch 20/20 1000/1000 [==============================] - 0s 23us/sample - loss: 0.1112 All the new log files will be created and we can keep track of our project and the parameters we used. OK, this model is not as good as the previous one, but fortunately we have all the logging information, so we don't have to remember the parameters we used for the best model. $ ls data/example_01/* data/example_01/train/* data/example_01/example_01.pkl data/example_01/train/example_01.20190823.213803.075040.1.stdout data/example_01/train/example_01.20190823.212609.830649.1.stderr data/example_01/train/example_01.parameters.20190823.212609.830649.1.yaml data/example_01/train/example_01.20190823.212609.830649.1.stdout data/example_01/train/example_01.parameters.20190823.213803.075040.1.yaml data/example_01/train/example_01.20190823.213803.075040.1.stderr","title":"Example 1: A neural network for \"XOR\""},{"location":"examples_custom_fucntions/#example-2-hyper-parameter-optimization","text":"Building on the previous example ( example_01.py and example_01.yaml ), let's assume that instead of trying to tune the learning_rate manually, we'd prefer to perform hyper-parameter optimization. In this example ( example_02 ), we'll set up hyper-parameter optimization on learning_rate . The python program remains exactly the same as in the previous example, we'll be adding a hyper-parameter optimization section to the YAML file. For the config YAML file (see example_02.yaml ), we jut add the following section: hyper_parameter_optimization: algorithm: 'tpe' max_evals: 100 space: model_train: learning_rate: ['uniform', 0.0, 0.5] We added a hyper_parameter_optimization section where we: - Define the hyper parameter algorithm ( tpe ) which is a Bayesian apprach - Set the number of evaluations to 100 - Define that we want to optimize the parameter learning_rate in the function @model_train using a uniform prior in the interval [0.0, 0.5] . We run the program: $ ./example_02.py -c example_02.yaml 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06<00:00, 1.44it/s, best loss: 0.07341234689950943] Here the hyper-parameter optimization is saying that the best loss found (with ten iterations) is 0.0734 . We also have all the parameter details, models, and STDOUT/STDERR for every single model created and trained: $ ls data/example_02/* data/example_02/train/* | cat data/example_02/example_02.pkl data/example_02/train/example_02.20190823.215947.132156.1.stderr data/example_02/train/example_02.20190823.215947.132156.1.stdout ... data/example_02/train/example_02.20190823.215953.151580.10.stderr data/example_02/train/example_02.20190823.215953.151580.10.stdout data/example_02/train/example_02.hyper_param_search.20190823.215953.151580.10.pkl data/example_02/train/example_02.parameters.20190823.215947.132156.1.yaml ... data/example_02/train/example_02.parameters.20190823.215953.151580.10.yaml","title":"Example 2: Hyper-parameter optimization"},{"location":"examples_custom_fucntions/#example-3-neural-network-architecture-optimization","text":"Now we build on the previous example (Example 2) by trying to optimize the neural network architecture. For this we just need to add a hyper parameter optimization when building the neural network (i.e. the @model_create step in the workflow). Simply add a line in the space definition within hyper_parameter_optimization section: The YAML is changed like this (see example_03.yaml ): hyper_parameter_optimization: ... space: model_create: num_neurons: ['randint', 5] ... Also we need a minor change in the python program is to ensure that we at least have one neuron in the hidden layer (otherwise the model doesn't make sense) So we add a single line to @model_create (see line num_neurons = max(num_neurons, 1) below): @model_create def my_model_create(dataset, num_neurons): model = Sequential() num_neurons = max(num_neurons, 1) # <-- Added this line model.add(Dense(num_neurons, activation='tanh', input_shape=(2,))) model.add(Dense(1, activation='tanh')) return model That's is, we have network architecture optimization ( num_neurons ) and hyper-parameter optimization ( learning_rate ). Let's run the program (output edited for readability): $ ./example_03.py -v -c example_03.yaml ... 2019-08-23 21:29:51,924 INFO Hyper parameter optimization: iteration: 10 ... best fit: 0.06886020198464393 best parameters: {'model_create': {'num_neurons': 3}, 'model_train': {'learning_rate': 0.22890998206259194}} ... The best parameters, for a 10 iteration hyper-optimization, are num_neurons=3 and learning_rate=0.2289 .","title":"Example 3: Neural network architecture optimization"},{"location":"explore/","text":"Explore (df) These steps implement data exploration when dataset is a DataFrame (i.e. dataset_type: 'df' ). The exploration step is run twice: Original (raw) dataset Pre-processed dataset With both analysis you can check whether the pre-processing steps had the desired results, such as properly imputing missing values, or normalizing the data All missing values are imputed on the pre-processed dataset, so any analysis related to missing values is only performed on the original (raw) dataset. Summary statistics Show DataFrame head and tail (firs and last lines) Summary statistics: count, mean, std, min, 25%, 50%, 75%, max Missing data analysis Number of missing values Plot: Percent of missing values Chart: DataFrame missingness Plot: Missing by column HeatMap: Missingness correlation Field description Field statistics Skewness, Kurtosis Normality test and p-value Log-Normality test and p-value Histogram and kernel density estimate Pair plots: scatter plot of variables pairs (up to plot_pairs_max , default 20) Correlation analysis Rank correlation (Spearsman) Top correlations, show correlation over corr_thresdld (default 0.7) Save top correlations as CSV file HeatMap Dendogram Dendogram of missing values Congig YAML These are the config_YAML options related to dataset exploration dataset_explore: # Set to 'false' to disable this step enable: true # Perform correlation analysis is_correlation_analysis: true # Show dendogram is_dendogram: true # Describe all variables is_describe_all: true # Perform 'missing data' analysis is_nas: true # Plot pairs of variables is_plot_pairs: true # Create summary is_summary: true # Also explore 'original' dataset (i.e. before transforming) is_use_ori: false # Consider variables 'highly correlated' if over this threshold corr_thresdld: 0.7 # Do not plot pairs if there are more than 'correlation_analysis_max' variables correlation_analysis_max: 100 # When plotting a histogram, the columns should have at least this number of # unique values to show the 'kernel density estimate' line describe_kde_min_uniq_values: 100 # Do not perform dendogram plot for more than 'dendogram_max' varaibles dendogram_max: 100 # Do not plot pairs if there are more than 'plot_pairs_max' varaibles plot_pairs_max: 20 # Consider a varaible 'normal' or 'log normal' if Shapiro-Wilks test is over this threshold shapiro_wilks_threshold: 0.1","title":"Workflow: Explore"},{"location":"explore/#explore-df","text":"These steps implement data exploration when dataset is a DataFrame (i.e. dataset_type: 'df' ). The exploration step is run twice: Original (raw) dataset Pre-processed dataset With both analysis you can check whether the pre-processing steps had the desired results, such as properly imputing missing values, or normalizing the data All missing values are imputed on the pre-processed dataset, so any analysis related to missing values is only performed on the original (raw) dataset.","title":"Explore (df)"},{"location":"explore/#summary-statistics","text":"Show DataFrame head and tail (firs and last lines) Summary statistics: count, mean, std, min, 25%, 50%, 75%, max","title":"Summary statistics"},{"location":"explore/#missing-data-analysis","text":"Number of missing values Plot: Percent of missing values Chart: DataFrame missingness Plot: Missing by column HeatMap: Missingness correlation","title":"Missing data analysis"},{"location":"explore/#field-description","text":"Field statistics Skewness, Kurtosis Normality test and p-value Log-Normality test and p-value Histogram and kernel density estimate","title":"Field description"},{"location":"explore/#pair-plots-scatter-plot-of-variables-pairs-up-to-plot_pairs_max-default-20","text":"","title":"Pair plots: scatter plot of variables pairs (up to plot_pairs_max, default 20)"},{"location":"explore/#correlation-analysis","text":"Rank correlation (Spearsman) Top correlations, show correlation over corr_thresdld (default 0.7) Save top correlations as CSV file HeatMap Dendogram Dendogram of missing values","title":"Correlation analysis"},{"location":"explore/#congig-yaml","text":"These are the config_YAML options related to dataset exploration dataset_explore: # Set to 'false' to disable this step enable: true # Perform correlation analysis is_correlation_analysis: true # Show dendogram is_dendogram: true # Describe all variables is_describe_all: true # Perform 'missing data' analysis is_nas: true # Plot pairs of variables is_plot_pairs: true # Create summary is_summary: true # Also explore 'original' dataset (i.e. before transforming) is_use_ori: false # Consider variables 'highly correlated' if over this threshold corr_thresdld: 0.7 # Do not plot pairs if there are more than 'correlation_analysis_max' variables correlation_analysis_max: 100 # When plotting a histogram, the columns should have at least this number of # unique values to show the 'kernel density estimate' line describe_kde_min_uniq_values: 100 # Do not perform dendogram plot for more than 'dendogram_max' varaibles dendogram_max: 100 # Do not plot pairs if there are more than 'plot_pairs_max' varaibles plot_pairs_max: 20 # Consider a varaible 'normal' or 'log normal' if Shapiro-Wilks test is over this threshold shapiro_wilks_threshold: 0.1","title":"Congig YAML"},{"location":"feature_importance/","text":"Workflow: Feature importance Feature importance tries to detemine which These steps implement data exploration when dataset is a DataFrame (i.e. dataset_type: 'df' ). Model-based column permutation Estimate feature importance based on a model by randomly permuting values on each column. How it works: Suffle a column and analyze how model performance is degraded. Most important features will make the model perform much worse when shuffled, unimportant features will not affect performance. Models used (SciKit-Learn, default parameters): - Random Forest - ExtraTrees - GradientBoosting Model-based drop-column Estimate feature importance based on a model by dropping a column. How it works: Drops a single column, re-train and analyze how model performance is degraded (respect to validation dataset). Most important features will make the model perform much worse when dropped, unimportant features will not affect performance Models used (SciKit-Learn, default parameters): - Random Forest - ExtraTrees - GradientBoosting SkLearn importance Use default SciKit-Learn importance estimation method. Models: Random Forest, ExtraTrees, GradientBoosting (default parameters) Regularization models Perform a model fir with regularization (using cross-validation), then analyze the model coefficients. Models used (SciKit-Learn, default parameters): - Lasso - Ridge - Lars (AIC) - Lars (BIC) Selection Use a SciKit-Learn SelectFdr or SelectKBest , depending on whether the selection function has p-values or not. Selection function Has p-value Classification / Regression f_regression True Regression mutual_info_regression False Regression f_classif True Classification mutual_info_classif False Classification chi^2 False Classification Recursive Feature Elimination Use recursive feature elimination (SciKit-Learn RFECV ) using several models: Model Classification / Regression Lasso Regression Ridge Regression Lars (AIC) Regression Lars (BIC) Regression Random Forest Classification, Regression ExtraTrees Classification, Regression GradientBoosting Classification, Regression In all cases, Scikit-Learn models with default parameters are used. Linear model p-value Calculate p-value based on a linear model ( statsmodels.regression.linear_model.OLS ). The null model variables can be set in the config_YAML , parameter linear_pvalue_null_model_variables . P-values are corrected for multiple testing using False Discovery Rate (FDR). Logistic regression p-value Calculate p-value based on a logistic regression model (Wilks theorem) ( statsmodels.discrete.discrete_model.Logit ). The null model variables can be set in the config_YAML , parameter logistic_regressions_by_class P-values are corrected for multiple testing using False Discovery Rate (FDR). When the classification is non-binary (i.e. multiple classes), the p-values are calculated as one class compared to all others (for each class). Then p-values are FDR adjusted. Significance is assessed on any comparison. Tree graph Create a shallow decision tree (default tree_graph_max_depth=4 ) and show a graph og the tree Weighted rank All feature importance algorithm results are combined in a summary table. Finally the results are ranked according to all methods: Model based methods are weighted according to the 'loss functions' from each model (on the validation set), i.e. lower is better. Losses (lower is better) are transformed to weights (higher is better) and corrected to be in the range [weight_min, weight_max] (defalt [1, 10] ) Other methods use a weight of weight_min (default 1.0) Ranks from all algorithms are multiplied by the weights to form a weighted rank sum ( rank_sum in the summary table) The rank_sum result is ranked, most important variables are shown first variable importance_permutation_RandomForest ... importance_dropcol_RandomForest ... ranks_sum rank_of_ranksum x1 73.424 ... 50.154 ... 134.99 1.0 x2 16.635 ... 11.455 ... 269.99 2.0 x3 2.723 ... 1.856 ... 404.98 3.0 Congig YAML These are the config_YAML options related to feature importance dataset_feature_importance: # Set to 'false' to disable this step enable: true # Set to 'false' to disable this step for 'na' dataset (i.e. a dataset of missing data) enable_na: true # Enable \"Feature Importance using Permutations\" (for different models) is_fip_random_forest: true is_fip_extra_trees: true is_fip_gradient_boosting: true # Regularization methods # Enable regularization methods (for different models) is_regularization_lasso: true is_regularization_ridge: true is_regularization_lars: true # Number of Cross-validation in regularization methods regularization_model_cv: 10 # Enable Recursive Feature Elimination (for different models) is_rfe_model: true is_rfe_model_lasso: true is_rfe_model_ridge: true is_rfe_model_lars_aic: true is_rfe_model_lars_bic: true is_rfe_model_random_forest: true is_rfe_model_extra_trees: true is_rfe_model_gradient_boosting: true rfe_model_cv: 0 # Number of Corss-validations in Recursive Feature Elimination methods # Enable model selection methods (SelectFdr / SelectKBest) is_select: true # Linear regression p-value is_linear_pvalue: true # Variables used for setting the null model (always add to linear regression model) linear_pvalue_null_model_variables: ['x6'] # Tree graph is_tree_graph: true tree_graph_max_depth: 4 # Number of layers to show in graph # Range to use when expanding weights # Note: Weights are converted to an interval [weight_min, weight_max] weight_max: 10.0 weight_min: 1.0 # Logistic regression p-value (Wilks) is_wilks: true # Variables used for setting the null model wilks_null_model_variables: ['age', 'sex', 'pc_1', 'pc_2', 'pc_3', 'pc_4']","title":"Workflow: Feature importance"},{"location":"feature_importance/#workflow-feature-importance","text":"Feature importance tries to detemine which These steps implement data exploration when dataset is a DataFrame (i.e. dataset_type: 'df' ).","title":"Workflow: Feature importance"},{"location":"feature_importance/#model-based-column-permutation","text":"Estimate feature importance based on a model by randomly permuting values on each column. How it works: Suffle a column and analyze how model performance is degraded. Most important features will make the model perform much worse when shuffled, unimportant features will not affect performance. Models used (SciKit-Learn, default parameters): - Random Forest - ExtraTrees - GradientBoosting","title":"Model-based column permutation"},{"location":"feature_importance/#model-based-drop-column","text":"Estimate feature importance based on a model by dropping a column. How it works: Drops a single column, re-train and analyze how model performance is degraded (respect to validation dataset). Most important features will make the model perform much worse when dropped, unimportant features will not affect performance Models used (SciKit-Learn, default parameters): - Random Forest - ExtraTrees - GradientBoosting","title":"Model-based drop-column"},{"location":"feature_importance/#sklearn-importance","text":"Use default SciKit-Learn importance estimation method. Models: Random Forest, ExtraTrees, GradientBoosting (default parameters)","title":"SkLearn importance"},{"location":"feature_importance/#regularization-models","text":"Perform a model fir with regularization (using cross-validation), then analyze the model coefficients. Models used (SciKit-Learn, default parameters): - Lasso - Ridge - Lars (AIC) - Lars (BIC)","title":"Regularization models"},{"location":"feature_importance/#selection","text":"Use a SciKit-Learn SelectFdr or SelectKBest , depending on whether the selection function has p-values or not. Selection function Has p-value Classification / Regression f_regression True Regression mutual_info_regression False Regression f_classif True Classification mutual_info_classif False Classification chi^2 False Classification","title":"Selection"},{"location":"feature_importance/#recursive-feature-elimination","text":"Use recursive feature elimination (SciKit-Learn RFECV ) using several models: Model Classification / Regression Lasso Regression Ridge Regression Lars (AIC) Regression Lars (BIC) Regression Random Forest Classification, Regression ExtraTrees Classification, Regression GradientBoosting Classification, Regression In all cases, Scikit-Learn models with default parameters are used.","title":"Recursive Feature Elimination"},{"location":"feature_importance/#linear-model-p-value","text":"Calculate p-value based on a linear model ( statsmodels.regression.linear_model.OLS ). The null model variables can be set in the config_YAML , parameter linear_pvalue_null_model_variables . P-values are corrected for multiple testing using False Discovery Rate (FDR).","title":"Linear model p-value"},{"location":"feature_importance/#logistic-regression-p-value","text":"Calculate p-value based on a logistic regression model (Wilks theorem) ( statsmodels.discrete.discrete_model.Logit ). The null model variables can be set in the config_YAML , parameter logistic_regressions_by_class P-values are corrected for multiple testing using False Discovery Rate (FDR). When the classification is non-binary (i.e. multiple classes), the p-values are calculated as one class compared to all others (for each class). Then p-values are FDR adjusted. Significance is assessed on any comparison.","title":"Logistic regression p-value"},{"location":"feature_importance/#tree-graph","text":"Create a shallow decision tree (default tree_graph_max_depth=4 ) and show a graph og the tree","title":"Tree graph"},{"location":"feature_importance/#weighted-rank","text":"All feature importance algorithm results are combined in a summary table. Finally the results are ranked according to all methods: Model based methods are weighted according to the 'loss functions' from each model (on the validation set), i.e. lower is better. Losses (lower is better) are transformed to weights (higher is better) and corrected to be in the range [weight_min, weight_max] (defalt [1, 10] ) Other methods use a weight of weight_min (default 1.0) Ranks from all algorithms are multiplied by the weights to form a weighted rank sum ( rank_sum in the summary table) The rank_sum result is ranked, most important variables are shown first variable importance_permutation_RandomForest ... importance_dropcol_RandomForest ... ranks_sum rank_of_ranksum x1 73.424 ... 50.154 ... 134.99 1.0 x2 16.635 ... 11.455 ... 269.99 2.0 x3 2.723 ... 1.856 ... 404.98 3.0","title":"Weighted rank"},{"location":"feature_importance/#congig-yaml","text":"These are the config_YAML options related to feature importance dataset_feature_importance: # Set to 'false' to disable this step enable: true # Set to 'false' to disable this step for 'na' dataset (i.e. a dataset of missing data) enable_na: true # Enable \"Feature Importance using Permutations\" (for different models) is_fip_random_forest: true is_fip_extra_trees: true is_fip_gradient_boosting: true # Regularization methods # Enable regularization methods (for different models) is_regularization_lasso: true is_regularization_ridge: true is_regularization_lars: true # Number of Cross-validation in regularization methods regularization_model_cv: 10 # Enable Recursive Feature Elimination (for different models) is_rfe_model: true is_rfe_model_lasso: true is_rfe_model_ridge: true is_rfe_model_lars_aic: true is_rfe_model_lars_bic: true is_rfe_model_random_forest: true is_rfe_model_extra_trees: true is_rfe_model_gradient_boosting: true rfe_model_cv: 0 # Number of Corss-validations in Recursive Feature Elimination methods # Enable model selection methods (SelectFdr / SelectKBest) is_select: true # Linear regression p-value is_linear_pvalue: true # Variables used for setting the null model (always add to linear regression model) linear_pvalue_null_model_variables: ['x6'] # Tree graph is_tree_graph: true tree_graph_max_depth: 4 # Number of layers to show in graph # Range to use when expanding weights # Note: Weights are converted to an interval [weight_min, weight_max] weight_max: 10.0 weight_min: 1.0 # Logistic regression p-value (Wilks) is_wilks: true # Variables used for setting the null model wilks_null_model_variables: ['age', 'sex', 'pc_1', 'pc_2', 'pc_3', 'pc_4']","title":"Congig YAML"},{"location":"install/","text":"Install The best way to install LogMl is to create a virtual environment and pull the code from GitHub: - Development version: master branch - Latest Release version: release-YYYYMMDDHHmmSS (release versions are identified by a timestamp: year, month, day, hour, minute, second) Requirements Operating system: LogMl was developed and tested on Linux and Mac (OS.X). Python 3.7 Virtual environment pip How to install Open a shell / terminal and follow these steps: # This creates a 'source' directory and pulls the latest (development) version from GitHub SRC_DIR=\"$HOME/workspace\" mkdir -p $SRC_DIR cd $SRC_DIR git clone https://github.com/AstraZeneca-NGS/LogMl.git # Running install.sh will install LogMl into the default directory ($HOME/logml) cd LogMl ./scripts/install.sh The scripts/install.sh script should take care of installing in a default directory ( $HOME/logml ). If you want another directory, just edit the script and change the INSTALL_DIR variable in the script.","title":"Install"},{"location":"install/#install","text":"The best way to install LogMl is to create a virtual environment and pull the code from GitHub: - Development version: master branch - Latest Release version: release-YYYYMMDDHHmmSS (release versions are identified by a timestamp: year, month, day, hour, minute, second)","title":"Install"},{"location":"install/#requirements","text":"Operating system: LogMl was developed and tested on Linux and Mac (OS.X). Python 3.7 Virtual environment pip","title":"Requirements"},{"location":"install/#how-to-install","text":"Open a shell / terminal and follow these steps: # This creates a 'source' directory and pulls the latest (development) version from GitHub SRC_DIR=\"$HOME/workspace\" mkdir -p $SRC_DIR cd $SRC_DIR git clone https://github.com/AstraZeneca-NGS/LogMl.git # Running install.sh will install LogMl into the default directory ($HOME/logml) cd LogMl ./scripts/install.sh The scripts/install.sh script should take care of installing in a default directory ( $HOME/logml ). If you want another directory, just edit the script and change the INSTALL_DIR variable in the script.","title":"How to install"},{"location":"introduction/","text":"Introduction To run LogMl you only need: A dataset A configuration (YAML) file Dataset By far the easiest is to use a csv file. Usually the file is loaded from data/{dataset_name}/{dataset_name}.csv , where dataset_name is some name (e.g. my_dataset ). Configuration file The configuration file is where you define basic information (e.g. the dataset name), which methods you want to run, algorithm parameters, etc. For example, this is the configuration file for one of the introductory examples (file config/intro.yaml ): dataset: # Section: Basic dataset information dataset_type: 'df' # This dataset is a data frame dataset_name: 'intro' # Dataset name dataset_path: 'data/intro' # Path to dataset directory outputs: ['y'] # Name of the output variables dataset_preprocess: # Section: Preprocessing options impute: # How to impute missing values median: true # Impute using 'median' normalize: # How to normalize inputs standard: true # Use 'standard', i.e. transform so that mean=0 and std=1 model: # Section: Model definition model_type: regression # It's a regression model model_name: 'model_intro' # Model name model_class: sklearn.ensemble.RandomForestRegressor # Use RandomForestRegressor from Scikit-learn model_path: 'data/intro/model' # Store models in this directory functions: # Section: User defined functions dataset_split: # Split dataset parameters split_validate: 0.2 # Use 20% for validation set split_test: 0.0 # No test set model_create: # Model parameters: Same parameters as SkLearn RandomForestRegressor n_estimators: 30 # Number of estimators min_samples_leaf: 3 # Minimum samples per leaf max_features: 0.7 # Max number of features Running LogMl Once the dataset and configuration files are ready, you can simply run LogMl like this: # Go to LogMl install dir, activate virtual environment cd ~/logml . ./bin/activate # Run LogMl # By default it will look for config file 'config.yaml' in the current directory ./src/logml.py","title":"Introduction"},{"location":"introduction/#introduction","text":"To run LogMl you only need: A dataset A configuration (YAML) file","title":"Introduction"},{"location":"introduction/#dataset","text":"By far the easiest is to use a csv file. Usually the file is loaded from data/{dataset_name}/{dataset_name}.csv , where dataset_name is some name (e.g. my_dataset ).","title":"Dataset"},{"location":"introduction/#configuration-file","text":"The configuration file is where you define basic information (e.g. the dataset name), which methods you want to run, algorithm parameters, etc. For example, this is the configuration file for one of the introductory examples (file config/intro.yaml ): dataset: # Section: Basic dataset information dataset_type: 'df' # This dataset is a data frame dataset_name: 'intro' # Dataset name dataset_path: 'data/intro' # Path to dataset directory outputs: ['y'] # Name of the output variables dataset_preprocess: # Section: Preprocessing options impute: # How to impute missing values median: true # Impute using 'median' normalize: # How to normalize inputs standard: true # Use 'standard', i.e. transform so that mean=0 and std=1 model: # Section: Model definition model_type: regression # It's a regression model model_name: 'model_intro' # Model name model_class: sklearn.ensemble.RandomForestRegressor # Use RandomForestRegressor from Scikit-learn model_path: 'data/intro/model' # Store models in this directory functions: # Section: User defined functions dataset_split: # Split dataset parameters split_validate: 0.2 # Use 20% for validation set split_test: 0.0 # No test set model_create: # Model parameters: Same parameters as SkLearn RandomForestRegressor n_estimators: 30 # Number of estimators min_samples_leaf: 3 # Minimum samples per leaf max_features: 0.7 # Max number of features","title":"Configuration file"},{"location":"introduction/#running-logml","text":"Once the dataset and configuration files are ready, you can simply run LogMl like this: # Go to LogMl install dir, activate virtual environment cd ~/logml . ./bin/activate # Run LogMl # By default it will look for config file 'config.yaml' in the current directory ./src/logml.py","title":"Running LogMl"},{"location":"introduction_cmd/","text":"Introduction In this introduction, we show an example on how to run LogMl from the command line Running from command line In order to run this example, you'll need to Copy the dataset to your LogMl install directory Copy the configuration file to your LogMl install directory Run LogMl Set up environment variables In the rest of the examples, we assume that the following variables are set to the corresponding directories: # I checked out LogMl GitHub repository to $HOME/workspace/LogMl # You need to change this variable accordingly LOGML_SRC=\"$HOME/workspace/LogMl\" # I installed LogMl in the default directory: $HOME/logml # You need to change this variable accordingly LOGML_INSTALL=\"$HOME/logml\" Copy dataset file The dataset in this example consists of three normally distributed variables ( x1 , x2 , x3 ) and some random noise ( n ), the output variable ( y ) is calculated as: y = 2 * x1 - 1 * x2 + 0.5 * x3 + 0.1 * n The file data/intro/intro.csv (from GitHub repository) is a CSV file with 1,000 samples from the above equation. Copy the file to your logml directory # Create data directory cd $LOGML_INSTALL mkdir -p data/intro # Copy dataset file from the source directory. cp $LOGML_SRC/data/intro/intro.csv data/intro/intro.csv Copy configuration file The YAML configuraton file for this example, is in GitHub's repository config/intro.yaml , you can copy it to the logml directory cd $LOGML_INSTALL mkdir -p config cp $LOGML_SRC/config/intro.yaml config/ Running LogMl To run LogMl, you just need to invoke LogMl with the propper configuration file # Activate virtual environment cd $LOGML_INSTALL . ./bin/activate # Run LogMl ./src/logml.py -c config/intro.yaml Results The output includes: Results: Dataset exploration Show dataset (head & tail) idx x1 x2 x3 y 1 0.977291 0.247040 0.272578 0.772232 2 1.261573 -0.090584 -0.386493 1.036706 3 0.442205 -1.059191 0.877819 0.986426 4 -1.353664 -0.310002 1.697421 -0.672185 5 0.744831 0.750820 0.754703 0.448015 Variables statistics (including normally analysis) variable x1 count 853 mean 1.77011e-17 std 1.00059 min -3.17445 25% -0.689298 50% -0.0406822 75% 0.663965 max 2.74289 unique 853 skewness 0.0353077 kurtosis -0.192718 Normality True Normality_test_pvalue 0.465024 Log_Normality False Log_Normality_test_pvalue 0 Variables distributions Pairs plots Correlation analysis Correlation dendogram Feature importance Feature importance: Model based RandomForest, ExtraTrees, GradientBoosting. Using drop column and permutation analysis Feature importance: Regression models Feature importance: Tree desition graph Feature importance: Weighted rank sum of all methods variable importance_permutation_RandomForest ... importance_dropcol_RandomForest ... ranks_sum rank_of_ranksum x1 73.424 ... 50.154 ... 134.99 1.0 x2 16.635 ... 11.455 ... 269.99 2.0 x3 2.723 ... 1.856 ... 404.98 3.0 Model search Model search with hyper-parameter tuning Summary of all models, ranked by validation performance Model train validation time sklearn.linear_model.TheilSenRegressor.20200104.020150.141139.143 0.0017 0.00144 0.89 sklearn.linear_model.LassoCV.20200104.020155.308599.151 0.0017 0.00145 0.05 sklearn.svm.LinearSVR.20200104.020005.137177.30 0.0017 0.00145 0.02 sklearn.linear_model.ARDRegression.20200104.020005.196092.32 0.0017 0.00146 3.88 sklearn.linear_model.BayesianRidge.20200104.020004.916091.22 0.0017 0.00146 0.01 sklearn.linear_model.Lars.20200104.020510.961392.460 0.0017 0.00146 0.00 sklearn.linear_model.LinearRegression.20200104.020006.495526.34 0.0017 0.00146 0.01 sklearn.linear_model.OrthogonalMatchingPursuit.20200104.020004.965360.24 0.0017 0.00146 0.01 sklearn.linear_model.RANSACRegressor.20200104.020155.257481.149 0.0017 0.00146 0.01 sklearn.linear_model.HuberRegressor.20200104.020155.202713.147 0.0017 0.00147 0.01 sklearn.ensemble.GradientBoostingRegressor.20200104.020331.408829.326 0.0035 0.00698 0.54 sklearn.ensemble.GradientBoostingRegressor.20200104.020333.410398.328 0.0048 0.00781 0.72 sklearn.ensemble.GradientBoostingRegressor.20200104.020337.272208.333 0.0036 0.00841 0.45 sklearn.linear_model.RidgeCV.20200104.020150.046718.139 0.0098 0.00926 0.01 sklearn.ensemble.GradientBoostingRegressor.20200104.020309.050059.304 0.0029 0.01083 0.98 ... sklearn.ensemble.GradientBoostingRegressor.20200104.020251.214883.275 1.0007 1.00914 0.51 sklearn.linear_model.Lasso.20200104.020155.407230.153 1.0 1.01526 0.00 sklearn.dummy.DummyRegressor.20200104.020004.830997.18 1.0 1.01526 0.01","title":"Introduction command line"},{"location":"introduction_cmd/#introduction","text":"In this introduction, we show an example on how to run LogMl from the command line","title":"Introduction"},{"location":"introduction_cmd/#running-from-command-line","text":"In order to run this example, you'll need to Copy the dataset to your LogMl install directory Copy the configuration file to your LogMl install directory Run LogMl","title":"Running from command line"},{"location":"introduction_cmd/#set-up-environment-variables","text":"In the rest of the examples, we assume that the following variables are set to the corresponding directories: # I checked out LogMl GitHub repository to $HOME/workspace/LogMl # You need to change this variable accordingly LOGML_SRC=\"$HOME/workspace/LogMl\" # I installed LogMl in the default directory: $HOME/logml # You need to change this variable accordingly LOGML_INSTALL=\"$HOME/logml\"","title":"Set up environment variables"},{"location":"introduction_cmd/#copy-dataset-file","text":"The dataset in this example consists of three normally distributed variables ( x1 , x2 , x3 ) and some random noise ( n ), the output variable ( y ) is calculated as: y = 2 * x1 - 1 * x2 + 0.5 * x3 + 0.1 * n The file data/intro/intro.csv (from GitHub repository) is a CSV file with 1,000 samples from the above equation. Copy the file to your logml directory # Create data directory cd $LOGML_INSTALL mkdir -p data/intro # Copy dataset file from the source directory. cp $LOGML_SRC/data/intro/intro.csv data/intro/intro.csv","title":"Copy dataset file"},{"location":"introduction_cmd/#copy-configuration-file","text":"The YAML configuraton file for this example, is in GitHub's repository config/intro.yaml , you can copy it to the logml directory cd $LOGML_INSTALL mkdir -p config cp $LOGML_SRC/config/intro.yaml config/","title":"Copy configuration file"},{"location":"introduction_cmd/#running-logml","text":"To run LogMl, you just need to invoke LogMl with the propper configuration file # Activate virtual environment cd $LOGML_INSTALL . ./bin/activate # Run LogMl ./src/logml.py -c config/intro.yaml","title":"Running LogMl"},{"location":"introduction_cmd/#results","text":"The output includes:","title":"Results"},{"location":"introduction_cmd/#results-dataset-exploration","text":"","title":"Results: Dataset exploration"},{"location":"introduction_cmd/#show-dataset-head-tail","text":"idx x1 x2 x3 y 1 0.977291 0.247040 0.272578 0.772232 2 1.261573 -0.090584 -0.386493 1.036706 3 0.442205 -1.059191 0.877819 0.986426 4 -1.353664 -0.310002 1.697421 -0.672185 5 0.744831 0.750820 0.754703 0.448015","title":"Show dataset (head &amp; tail)"},{"location":"introduction_cmd/#variables-statistics-including-normally-analysis","text":"variable x1 count 853 mean 1.77011e-17 std 1.00059 min -3.17445 25% -0.689298 50% -0.0406822 75% 0.663965 max 2.74289 unique 853 skewness 0.0353077 kurtosis -0.192718 Normality True Normality_test_pvalue 0.465024 Log_Normality False Log_Normality_test_pvalue 0","title":"Variables statistics (including normally analysis)"},{"location":"introduction_cmd/#variables-distributions","text":"","title":"Variables distributions"},{"location":"introduction_cmd/#pairs-plots","text":"","title":"Pairs plots"},{"location":"introduction_cmd/#correlation-analysis","text":"","title":"Correlation analysis"},{"location":"introduction_cmd/#correlation-dendogram","text":"","title":"Correlation dendogram"},{"location":"introduction_cmd/#feature-importance","text":"","title":"Feature importance"},{"location":"introduction_cmd/#feature-importance-model-based","text":"RandomForest, ExtraTrees, GradientBoosting. Using drop column and permutation analysis","title":"Feature importance: Model based"},{"location":"introduction_cmd/#feature-importance-regression-models","text":"","title":"Feature importance: Regression models"},{"location":"introduction_cmd/#feature-importance-tree-desition-graph","text":"","title":"Feature importance: Tree desition graph"},{"location":"introduction_cmd/#feature-importance-weighted-rank-sum-of-all-methods","text":"variable importance_permutation_RandomForest ... importance_dropcol_RandomForest ... ranks_sum rank_of_ranksum x1 73.424 ... 50.154 ... 134.99 1.0 x2 16.635 ... 11.455 ... 269.99 2.0 x3 2.723 ... 1.856 ... 404.98 3.0","title":"Feature importance: Weighted rank sum of all methods"},{"location":"introduction_cmd/#model-search","text":"","title":"Model search"},{"location":"introduction_cmd/#model-search-with-hyper-parameter-tuning","text":"Summary of all models, ranked by validation performance Model train validation time sklearn.linear_model.TheilSenRegressor.20200104.020150.141139.143 0.0017 0.00144 0.89 sklearn.linear_model.LassoCV.20200104.020155.308599.151 0.0017 0.00145 0.05 sklearn.svm.LinearSVR.20200104.020005.137177.30 0.0017 0.00145 0.02 sklearn.linear_model.ARDRegression.20200104.020005.196092.32 0.0017 0.00146 3.88 sklearn.linear_model.BayesianRidge.20200104.020004.916091.22 0.0017 0.00146 0.01 sklearn.linear_model.Lars.20200104.020510.961392.460 0.0017 0.00146 0.00 sklearn.linear_model.LinearRegression.20200104.020006.495526.34 0.0017 0.00146 0.01 sklearn.linear_model.OrthogonalMatchingPursuit.20200104.020004.965360.24 0.0017 0.00146 0.01 sklearn.linear_model.RANSACRegressor.20200104.020155.257481.149 0.0017 0.00146 0.01 sklearn.linear_model.HuberRegressor.20200104.020155.202713.147 0.0017 0.00147 0.01 sklearn.ensemble.GradientBoostingRegressor.20200104.020331.408829.326 0.0035 0.00698 0.54 sklearn.ensemble.GradientBoostingRegressor.20200104.020333.410398.328 0.0048 0.00781 0.72 sklearn.ensemble.GradientBoostingRegressor.20200104.020337.272208.333 0.0036 0.00841 0.45 sklearn.linear_model.RidgeCV.20200104.020150.046718.139 0.0098 0.00926 0.01 sklearn.ensemble.GradientBoostingRegressor.20200104.020309.050059.304 0.0029 0.01083 0.98 ... sklearn.ensemble.GradientBoostingRegressor.20200104.020251.214883.275 1.0007 1.00914 0.51 sklearn.linear_model.Lasso.20200104.020155.407230.153 1.0 1.01526 0.00 sklearn.dummy.DummyRegressor.20200104.020004.830997.18 1.0 1.01526 0.01","title":"Model search with hyper-parameter tuning"},{"location":"introduction_juppyter/","text":"Introduction In this introduction, we show an example on how to run LogMl from a Jupyter Notebook The source notebook can be found in notebooks/intro_bare.ipynb (source code from GitHub) Running from Jupyter Notebooks In order to run this example, you'll need to Set up your Jupyter Notebook's LogMl environment (if you never did it before) Copy the dataset to your LogMl install directory Copy the configuration file to your LogMl install directory Run LogMl from a new Jupyter Notebook Set up environment variables In the rest of the examples, we assume that the following variables are set to the corresponding directories: # I checked out LogMl GitHub repository to $HOME/workspace/LogMl # You need to change this variable accordingly LOGML_SRC=\"$HOME/workspace/LogMl\" # I installed LogMl in the default directory: $HOME/logml # You need to change this variable accordingly LOGML_INSTALL=\"$HOME/logml\" Set up Jupyter Notebooks environment The first step is to make sure the LogMl virtual environment is available when you run Jupyter Notebooks You can do it by running these commands: # Activate virtual environment cd $LOGML_INSTALL . ./bin/activate # Make sure you have `ipykernel` installed, otherwise you can run the following line pip install ipykernel # Add kernel to Jupyter Notebooks python -m ipykernel install --name=logml Copy dataset file The dataset in this example consists of three normally distributed variables ( x1 , x2 , x3 ) and some random noise ( n ), the output variable ( y ) is calculated as: y = 2 * x1 - 1 * x2 + 0.5 * x3 + 0.1 * n The file data/intro/intro.csv (from GitHub repository) is a CSV file with 1,000 samples from the above equation. Copy the file to your logml directory # Create data directory cd $LOGML_INSTALL mkdir -p data/intro # Copy dataset file from the source directory. cp $LOGML_SRC/data/intro/intro.csv data/intro/intro.csv Copy configuration file The YAML configuraton file for this example, is in GitHub's repository config/intro.yaml , you can copy it to the logml directory $ cd $LOGML_INSTALL $ mkdir -p config $ cp $LOGML_SRC/config/intro.yaml config/ Running LogMl To run LogMl, create a new Jupyter notebook, making sure to select logml environment. Then (from the new NoteBook) you typically run a cell to configure some defaults: # Show plots in the notebook %matplotlib inline # Set path to use 'src' subdir import os, sys from pathlib import Path logml_src = str(Path(os.getcwd())/'src') sys.path.append(logml_src) After that, all you need to do is to create a LogMl object and run it from logml import * ml = LogMl('config/intro.yaml') ml() Results Click here to see LogMl's output Or here to see the full notebook that creates the dataset and explains more details Dataset exploration Show dataset (head & tail) Variables statistics (including normally analysis) Variables distributions Pairs plots Correlation analysis Correlation dendogram Feature importance Model based: RandomForest, ExtraTrees, GradientBoosting. Using drop column and permutation analysis Regression models Tree decition graph Weighted rank sum of all methods Model search with hyper-parameter tunning Summary of all models, ranked by validation performance","title":"Introduction Jupyter Notebooks"},{"location":"introduction_juppyter/#introduction","text":"In this introduction, we show an example on how to run LogMl from a Jupyter Notebook The source notebook can be found in notebooks/intro_bare.ipynb (source code from GitHub)","title":"Introduction"},{"location":"introduction_juppyter/#running-from-jupyter-notebooks","text":"In order to run this example, you'll need to Set up your Jupyter Notebook's LogMl environment (if you never did it before) Copy the dataset to your LogMl install directory Copy the configuration file to your LogMl install directory Run LogMl from a new Jupyter Notebook","title":"Running from Jupyter Notebooks"},{"location":"introduction_juppyter/#set-up-environment-variables","text":"In the rest of the examples, we assume that the following variables are set to the corresponding directories: # I checked out LogMl GitHub repository to $HOME/workspace/LogMl # You need to change this variable accordingly LOGML_SRC=\"$HOME/workspace/LogMl\" # I installed LogMl in the default directory: $HOME/logml # You need to change this variable accordingly LOGML_INSTALL=\"$HOME/logml\"","title":"Set up environment variables"},{"location":"introduction_juppyter/#set-up-jupyter-notebooks-environment","text":"The first step is to make sure the LogMl virtual environment is available when you run Jupyter Notebooks You can do it by running these commands: # Activate virtual environment cd $LOGML_INSTALL . ./bin/activate # Make sure you have `ipykernel` installed, otherwise you can run the following line pip install ipykernel # Add kernel to Jupyter Notebooks python -m ipykernel install --name=logml","title":"Set up Jupyter Notebooks environment"},{"location":"introduction_juppyter/#copy-dataset-file","text":"The dataset in this example consists of three normally distributed variables ( x1 , x2 , x3 ) and some random noise ( n ), the output variable ( y ) is calculated as: y = 2 * x1 - 1 * x2 + 0.5 * x3 + 0.1 * n The file data/intro/intro.csv (from GitHub repository) is a CSV file with 1,000 samples from the above equation. Copy the file to your logml directory # Create data directory cd $LOGML_INSTALL mkdir -p data/intro # Copy dataset file from the source directory. cp $LOGML_SRC/data/intro/intro.csv data/intro/intro.csv","title":"Copy dataset file"},{"location":"introduction_juppyter/#copy-configuration-file","text":"The YAML configuraton file for this example, is in GitHub's repository config/intro.yaml , you can copy it to the logml directory $ cd $LOGML_INSTALL $ mkdir -p config $ cp $LOGML_SRC/config/intro.yaml config/","title":"Copy configuration file"},{"location":"introduction_juppyter/#running-logml","text":"To run LogMl, create a new Jupyter notebook, making sure to select logml environment. Then (from the new NoteBook) you typically run a cell to configure some defaults: # Show plots in the notebook %matplotlib inline # Set path to use 'src' subdir import os, sys from pathlib import Path logml_src = str(Path(os.getcwd())/'src') sys.path.append(logml_src) After that, all you need to do is to create a LogMl object and run it from logml import * ml = LogMl('config/intro.yaml') ml()","title":"Running LogMl"},{"location":"introduction_juppyter/#results","text":"Click here to see LogMl's output Or here to see the full notebook that creates the dataset and explains more details Dataset exploration Show dataset (head & tail) Variables statistics (including normally analysis) Variables distributions Pairs plots Correlation analysis Correlation dendogram Feature importance Model based: RandomForest, ExtraTrees, GradientBoosting. Using drop column and permutation analysis Regression models Tree decition graph Weighted rank sum of all methods Model search with hyper-parameter tunning Summary of all models, ranked by validation performance","title":"Results"},{"location":"logging/","text":"Logging LogMl automatically logs a lot of information so you can retrieve, debug, and reproduce your experiments. Datasets : Datasets are saved to (pickle) files after pre-processing and augmentation. The next time you run LogMl on the same dataset, LogMl checks if there is a Processed_dataset_file and loads it, this can save significant processing, particularly if you re-run LogMl many times (e.g. when fine-tuning a model). This can be customized by a user_defined_function . Parameters : Parameters from config_YAML are stored for future references. Even if you change the original config_YAML , the original parameters are saved to a YAML log file.Note that user_defined_functions can have parameters defined in a YAML file, this makes it convenient for repeatability, since YAML file is logged for each model. Models : All models are saved to a (pickle) file. This can be customized by a user_defined_function . Model training output : All output (STDOUT and STDERR) from model training is redirected to log files. This can be very useful to debug models. Plots and charts : Plots are by default saved to the logml_plots directory as PNG images Summary data : Summary tables and outputs are stored as CSV files for further analysis and references (e.g. feature importance tables, model training tables, correlation rankings, feature importance weights, dot graphs, etc.)","title":"Logging"},{"location":"logging/#logging","text":"LogMl automatically logs a lot of information so you can retrieve, debug, and reproduce your experiments. Datasets : Datasets are saved to (pickle) files after pre-processing and augmentation. The next time you run LogMl on the same dataset, LogMl checks if there is a Processed_dataset_file and loads it, this can save significant processing, particularly if you re-run LogMl many times (e.g. when fine-tuning a model). This can be customized by a user_defined_function . Parameters : Parameters from config_YAML are stored for future references. Even if you change the original config_YAML , the original parameters are saved to a YAML log file.Note that user_defined_functions can have parameters defined in a YAML file, this makes it convenient for repeatability, since YAML file is logged for each model. Models : All models are saved to a (pickle) file. This can be customized by a user_defined_function . Model training output : All output (STDOUT and STDERR) from model training is redirected to log files. This can be very useful to debug models. Plots and charts : Plots are by default saved to the logml_plots directory as PNG images Summary data : Summary tables and outputs are stored as CSV files for further analysis and references (e.g. feature importance tables, model training tables, correlation rankings, feature importance weights, dot graphs, etc.)","title":"Logging"},{"location":"model/","text":"Workflow: Model In these steps we create and train models. This also takes care of common tasks, such as hyper-parameter optimization, cross-validation and model analysis. The main steps are: model_create : Model Create model_train : Model Train model_save : Model Save Model Save train results model_evaluate : Model Test model_evaluate : Model Validate A new model_id is created each time a new model is created/trained. This is used to make sure that files created during a run do not collision with other files names from previous runs. The model_id has the format yyyymmdd_hhmmss.counter where: - yyyy , mm , dd , hh , mm , ss : Current year, month, day, hour, minute, second (UTC time) - counter : Number of models created in this Log(ML) run (increasing counter starting with 1 ). Logging : All results from STDOUT and STDERR are saved to {model_path}/{model_name}.parameters.{model_id}.stdout and {model_path}/{model_name}.parameters.{model_id}.stderr respectively. Note that model_id is included in the path, so creating several models in the same Log(ML) run would save each output set to different stdout/stderr files (see details below). Model: Create Create a new model, to be trained. It also saves the parameters used to create the model to a YAML file. If a user defined function decorated with @model_create exists, it is invoked If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, the program exits with an error. Parameters: The first parameter is dataset_train if the dataset was split, otherwise is the full dataset. Other parameters are defined in config_YAML file section functions , sub-section model_create The return value from the user defined function is stored as the model Current parameters are saved to a YAML file {model_path}/{model_name}.parameters.{model_id}.yaml . Note that model_id is included in the path, so creating several models in the same Log(ML) run would save each parameter set to different YAML files. Model: Train Train the model. If a user defined function decorated with @model_train exists, it is invoked If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, the program exits with an error. Parameters: The first parameters are model and dataset_train (if the dataset was split, otherwise is the full dataset). Other parameters are defined in config_YAML file section functions , sub-section model_train The return value from the user defined function is stored as the train_results (these result will be saved, see later steps) Model: Save Save the (trained) model. If a user defined function decorated with @model_save exists, it is invoked If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, the program tries to save using a pickle file (see next step). Parameters: The first parameters is the model . Other parameters are defined in config_YAML file section functions , sub-section model_save Return successful Attempt to save model to pickle file if previous step ( @model_save function) failed. If parameter is_save_model_pickle from config_YAML file is set to False , this step is skipped The model resulting from training is saved to a pickle file file {model_path}/{model_name}.model.{model_id}.pkl . Note that model_id is included in the path, so training several models in the same Log(ML) run would save each model to different pickle files. Attempt to save model to using model.save() if previous step failed. If parameter is_save_model_method from config_YAML file is set to False , this step is skipped Invoke model's method model.save({file_name}) , where file_name is set to {model_path}/{model_name}.model.{model_id}.{is_save_model_method_ext} (parameter is_save_model_method_ext is defined in config_YAML file) Model: Save train Results Save results from training to a pickle file Attempt to save model training results (i.e. the return value from @model_train function) to pickle. If parameter is_save_train_pickle from config_YAML file is set to False , this step is skipped If the results are None , this step is skipped The model resulting from training is saved to a pickle file file {model_path}/{model_name}.train_results.{model_id}.pkl . Note that model_id is included in the path, so training several models in the same Log(ML) run would save train results to different pickle files. Model: Test Evaluate the model on the dataset_test dataset_test If a user defined function decorated with @model_evaluate exists, it is invoked If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, the program exits with an error. Parameters: The first parameters are model and dataset_test (if the dataset was split, otherwise use full dataset). Other parameters are defined in config_YAML file section functions , sub-section model_evaluate The return value from the user defined function is stored as the test_results (these result will be saved, see later steps) Model: Save test results Attempt to save model test results (i.e. the return value from @model_evaluate function invoked with dataset_test parameter) to pickle. If parameter is_save_test_pickle from config_YAML file is set to False , this step is skipped If the results are None , this step is skipped The model resulting from training is saved to a pickle file file {model_path}/{model_name}.test_results.{model_id}.pkl . Note that model_id is included in the path, so testing several models in the same Log(ML) run would save train results to different pickle files. Model: Validate Evaluate the model on the dataset_validate dataset_test If a user defined function decorated with @model_evaluate exists, it is invoked If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, the program exits with an error. Parameters: The first parameters are model and dataset_validate (if the dataset was split, otherwise this step fails). Other parameters are defined in config_YAML file section functions , sub-section model_evaluate The return value from the user defined function is stored as the validate_results (these result will be saved, see later steps) Model: Save validate results Attempt to save model test results (i.e. the return value from @model_evaluate function invoked with dataset_validate parameter) to pickle. If parameter is_save_validate_pickle from config_YAML file is set to False , this step is skipped If the results are None , this step is skipped The model resulting from training is saved to a pickle file file {model_path}/{model_name}.validate_results.{model_id}.pkl . Note that model_id is included in the path, so validating several models in the same Log(ML) run would save train results to different pickle files.","title":"Workflow: Model"},{"location":"model/#workflow-model","text":"In these steps we create and train models. This also takes care of common tasks, such as hyper-parameter optimization, cross-validation and model analysis. The main steps are: model_create : Model Create model_train : Model Train model_save : Model Save Model Save train results model_evaluate : Model Test model_evaluate : Model Validate A new model_id is created each time a new model is created/trained. This is used to make sure that files created during a run do not collision with other files names from previous runs. The model_id has the format yyyymmdd_hhmmss.counter where: - yyyy , mm , dd , hh , mm , ss : Current year, month, day, hour, minute, second (UTC time) - counter : Number of models created in this Log(ML) run (increasing counter starting with 1 ). Logging : All results from STDOUT and STDERR are saved to {model_path}/{model_name}.parameters.{model_id}.stdout and {model_path}/{model_name}.parameters.{model_id}.stderr respectively. Note that model_id is included in the path, so creating several models in the same Log(ML) run would save each output set to different stdout/stderr files (see details below).","title":"Workflow: Model"},{"location":"model/#model-create","text":"Create a new model, to be trained. It also saves the parameters used to create the model to a YAML file. If a user defined function decorated with @model_create exists, it is invoked If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, the program exits with an error. Parameters: The first parameter is dataset_train if the dataset was split, otherwise is the full dataset. Other parameters are defined in config_YAML file section functions , sub-section model_create The return value from the user defined function is stored as the model Current parameters are saved to a YAML file {model_path}/{model_name}.parameters.{model_id}.yaml . Note that model_id is included in the path, so creating several models in the same Log(ML) run would save each parameter set to different YAML files.","title":"Model: Create"},{"location":"model/#model-train","text":"Train the model. If a user defined function decorated with @model_train exists, it is invoked If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, the program exits with an error. Parameters: The first parameters are model and dataset_train (if the dataset was split, otherwise is the full dataset). Other parameters are defined in config_YAML file section functions , sub-section model_train The return value from the user defined function is stored as the train_results (these result will be saved, see later steps)","title":"Model: Train"},{"location":"model/#model-save","text":"Save the (trained) model. If a user defined function decorated with @model_save exists, it is invoked If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, the program tries to save using a pickle file (see next step). Parameters: The first parameters is the model . Other parameters are defined in config_YAML file section functions , sub-section model_save Return successful Attempt to save model to pickle file if previous step ( @model_save function) failed. If parameter is_save_model_pickle from config_YAML file is set to False , this step is skipped The model resulting from training is saved to a pickle file file {model_path}/{model_name}.model.{model_id}.pkl . Note that model_id is included in the path, so training several models in the same Log(ML) run would save each model to different pickle files. Attempt to save model to using model.save() if previous step failed. If parameter is_save_model_method from config_YAML file is set to False , this step is skipped Invoke model's method model.save({file_name}) , where file_name is set to {model_path}/{model_name}.model.{model_id}.{is_save_model_method_ext} (parameter is_save_model_method_ext is defined in config_YAML file)","title":"Model: Save"},{"location":"model/#model-save-train-results","text":"Save results from training to a pickle file Attempt to save model training results (i.e. the return value from @model_train function) to pickle. If parameter is_save_train_pickle from config_YAML file is set to False , this step is skipped If the results are None , this step is skipped The model resulting from training is saved to a pickle file file {model_path}/{model_name}.train_results.{model_id}.pkl . Note that model_id is included in the path, so training several models in the same Log(ML) run would save train results to different pickle files.","title":"Model: Save train Results"},{"location":"model/#model-test","text":"Evaluate the model on the dataset_test dataset_test If a user defined function decorated with @model_evaluate exists, it is invoked If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, the program exits with an error. Parameters: The first parameters are model and dataset_test (if the dataset was split, otherwise use full dataset). Other parameters are defined in config_YAML file section functions , sub-section model_evaluate The return value from the user defined function is stored as the test_results (these result will be saved, see later steps)","title":"Model: Test"},{"location":"model/#model-save-test-results","text":"Attempt to save model test results (i.e. the return value from @model_evaluate function invoked with dataset_test parameter) to pickle. If parameter is_save_test_pickle from config_YAML file is set to False , this step is skipped If the results are None , this step is skipped The model resulting from training is saved to a pickle file file {model_path}/{model_name}.test_results.{model_id}.pkl . Note that model_id is included in the path, so testing several models in the same Log(ML) run would save train results to different pickle files.","title":"Model: Save test results"},{"location":"model/#model-validate","text":"Evaluate the model on the dataset_validate dataset_test If a user defined function decorated with @model_evaluate exists, it is invoked If there is no function or the section is disabled in the config file (i.e. enable=False ), this step has failed, the program exits with an error. Parameters: The first parameters are model and dataset_validate (if the dataset was split, otherwise this step fails). Other parameters are defined in config_YAML file section functions , sub-section model_evaluate The return value from the user defined function is stored as the validate_results (these result will be saved, see later steps)","title":"Model: Validate"},{"location":"model/#model-save-validate-results","text":"Attempt to save model test results (i.e. the return value from @model_evaluate function invoked with dataset_validate parameter) to pickle. If parameter is_save_validate_pickle from config_YAML file is set to False , this step is skipped If the results are None , this step is skipped The model resulting from training is saved to a pickle file file {model_path}/{model_name}.validate_results.{model_id}.pkl . Note that model_id is included in the path, so validating several models in the same Log(ML) run would save train results to different pickle files.","title":"Model: Save validate results"},{"location":"model_hyperopt/","text":"Hyper-parameter optimization This workflow allows to perform hyper-parameter optimization using a Bayesian framework ( hyper-opt ). The hyper parameters can be optimized in several stages of the \"dataset\" and \"model\". The hyper-parameter optimization workflow adds a bayesian optimization on top of the main workflow. This means that, conceptually, it's executing the main workflow several times using a bayesian optimizer. The hyper-parameter optimizaition method used is HyperOpt, for details see Hyperopt documentation Typically, hyper-parameter optimization is used to tune model training parameters. Log(ML) also allows to tune model creation parameters, as well as data augmentation and preprocessing parameters. YAML config YAML configuration of hyper parameter optimization: All parameter are defined in the hyper_parameter_optimization section. hyper_parameter_optimization: enable: False # Set this to 'True' or comment out to enable hyper-parameter optimization show_progressbar: True # Show progress bar algorithm: 'tpe' # Algorithm: 'tpe' (Bayesian Tree of Parzen Estimators), 'random' (random search) max_evals: 100 # Max number of hyper-parameter evaluations. Keep in mnd that each evaluation is a full model training # Parameter space to explore space: # Parameters search space specification, add one section for each user defined function you want to optimize dataset_augment: # Add parameter space specification for each part you want to optimize (see examples below) dataset_create: dataset_preprocess: model_create: model_train: Search space : We define parameters for each part we want to optimize (e.g. preprocess , model_create , etc.). The format for each parameter space is: parameter_name: ['distribution', distribution)parameters...] For distribution names and parameters, see: section 'Parameter Expressions' Important: The parameters space definition should be a subset of the parameters in each function section. Examples Perform hyper-parameter optimization of the learning rate using a uniform distribution as a p hyper_parameter_optimization: algorithm: 'tpe' max_evals: 100 space: model_train: learning_rate: ['uniform', 0.0, 0.5] Hyper parameter optimization not only can be used with model_train , but also with other named_workflow_steps , such as model_create hyper_parameter_optimization: algorithm: 'tpe' max_evals: 100 space: model_create: layer_1: ['randint', 10] model_train: learning_rate: ['uniform', 0.0, 0.5] You can even perform hyper-parameter optimization including the dataset_preprocess named_workflow_step hyper_parameter_optimization: algorithm: 'tpe' max_evals: 100 space: dataset_preprocess: num_x: ['choice', [100, 200, 300, 400, 500]] num_y: ['choice', [20, 50, 100, 200]] model_create: layer_1: ['randint', 20] layer_2: ['randint', 10] model_train: learning_rate: ['uniform', 0.0, 0.5]","title":"Workflow: Hyper-parameter tunnig"},{"location":"model_hyperopt/#hyper-parameter-optimization","text":"This workflow allows to perform hyper-parameter optimization using a Bayesian framework ( hyper-opt ). The hyper parameters can be optimized in several stages of the \"dataset\" and \"model\". The hyper-parameter optimization workflow adds a bayesian optimization on top of the main workflow. This means that, conceptually, it's executing the main workflow several times using a bayesian optimizer. The hyper-parameter optimizaition method used is HyperOpt, for details see Hyperopt documentation Typically, hyper-parameter optimization is used to tune model training parameters. Log(ML) also allows to tune model creation parameters, as well as data augmentation and preprocessing parameters.","title":"Hyper-parameter optimization"},{"location":"model_hyperopt/#yaml-config","text":"YAML configuration of hyper parameter optimization: All parameter are defined in the hyper_parameter_optimization section. hyper_parameter_optimization: enable: False # Set this to 'True' or comment out to enable hyper-parameter optimization show_progressbar: True # Show progress bar algorithm: 'tpe' # Algorithm: 'tpe' (Bayesian Tree of Parzen Estimators), 'random' (random search) max_evals: 100 # Max number of hyper-parameter evaluations. Keep in mnd that each evaluation is a full model training # Parameter space to explore space: # Parameters search space specification, add one section for each user defined function you want to optimize dataset_augment: # Add parameter space specification for each part you want to optimize (see examples below) dataset_create: dataset_preprocess: model_create: model_train: Search space : We define parameters for each part we want to optimize (e.g. preprocess , model_create , etc.). The format for each parameter space is: parameter_name: ['distribution', distribution)parameters...] For distribution names and parameters, see: section 'Parameter Expressions' Important: The parameters space definition should be a subset of the parameters in each function section.","title":"YAML config"},{"location":"model_hyperopt/#examples","text":"Perform hyper-parameter optimization of the learning rate using a uniform distribution as a p hyper_parameter_optimization: algorithm: 'tpe' max_evals: 100 space: model_train: learning_rate: ['uniform', 0.0, 0.5] Hyper parameter optimization not only can be used with model_train , but also with other named_workflow_steps , such as model_create hyper_parameter_optimization: algorithm: 'tpe' max_evals: 100 space: model_create: layer_1: ['randint', 10] model_train: learning_rate: ['uniform', 0.0, 0.5] You can even perform hyper-parameter optimization including the dataset_preprocess named_workflow_step hyper_parameter_optimization: algorithm: 'tpe' max_evals: 100 space: dataset_preprocess: num_x: ['choice', [100, 200, 300, 400, 500]] num_y: ['choice', [20, 50, 100, 200]] model_create: layer_1: ['randint', 20] layer_2: ['randint', 10] model_train: learning_rate: ['uniform', 0.0, 0.5]","title":"Examples"},{"location":"model_search/","text":"Model Search LogMl can perform model search, that is, fit many different models and rank them according to their (validation) performance. Models used in model search (these models are based on SciKit-Learn): sklearn.dummy.DummyClassifier sklearn.dummy.DummyRegressor sklearn.ensemble.AdaBoostClassifier sklearn.ensemble.AdaBoostRegressor sklearn.ensemble.BaggingClassifier sklearn.ensemble.BaggingRegressor sklearn.ensemble.ExtraTreesClassifier sklearn.ensemble.ExtraTreesRegressor sklearn.ensemble.GradientBoostingClassifier sklearn.ensemble.GradientBoostingRegressor sklearn.ensemble.HistGradientBoostingClassifier sklearn.ensemble.HistGradientBoostingRegressor sklearn.ensemble.RandomForestClassifier sklearn.ensemble.RandomForestRegressor sklearn.linear_model.ARDRegression sklearn.linear_model.BayesianRidge sklearn.linear_model.HuberRegressor sklearn.linear_model.Lars sklearn.linear_model.Lasso sklearn.linear_model.LassoCV sklearn.linear_model.LinearRegression sklearn.linear_model.LogisticRegressionCV sklearn.linear_model.OrthogonalMatchingPursuit sklearn.linear_model.PassiveAggressiveClassifier sklearn.linear_model.Perceptron sklearn.linear_model.RANSACRegressor sklearn.linear_model.Ridge sklearn.linear_model.RidgeCV sklearn.linear_model.TheilSenRegressor sklearn.naive_bayes.BernoulliNB sklearn.naive_bayes.ComplementNB sklearn.naive_bayes.GaussianNB sklearn.naive_bayes.MultinomialNB sklearn.neighbors.KNeighborsClassifier sklearn.neighbors.KNeighborsRegressor sklearn.neighbors.NearestCentroid sklearn.neighbors.RadiusNeighborsClassifier sklearn.neighbors.RadiusNeighborsRegressor sklearn.svm.LinearSVC sklearn.svm.LinearSVR sklearn.svm.NuSVC sklearn.svm.NuSVR sklearn.svm.SVC sklearn.svm.SVR sklearn.tree.DecisionTreeClassifier sklearn.tree.DecisionTreeRegressor Note that Dummy models are included. Dummy models are useful as a simple baseline to compare with other (real) models. Model Search with Hyper-parameter optimization: Hyper-parameter optimization can be used in conjunction with model search for find a suitable set of hyper-parameters for each model. These are the models currently using hyper-parameter optimization in model search. sklearn.ensemble.ExtraTreesClassifier sklearn.ensemble.ExtraTreesRegressor sklearn.ensemble.GradientBoostingClassifier sklearn.ensemble.GradientBoostingRegressor sklearn.neighbors.KNeighborsClassifier sklearn.neighbors.KNeighborsRegressor sklearn.ensemble.RandomForestClassifier sklearn.ensemble.RandomForestRegressor Results Model search results are shown in a table, ranked by validation score. The table also shows: - train loss - validation loss - training time - all hyper-parameters used to create the model (NA if the hyper-parameters does not apply to a specific model) Config_YAML The config_YAML pard that refers to model search simply includes all model file YAMLs in a sub directory: model_search: enable: true models: !include models_search/*.yaml Each YAML file in the sub-directory models_search is a configuration for a specific model. For instance, here is the configuration YAML for RandomForestRegressor, you can see how default parameter values and hyper parameter optimization: RandomForestRegressor: functions: model_create: bootstrap: true criterion: mse max_features: auto min_impurity_decrease: 0.0 min_samples_leaf: 1 min_samples_split: 2 min_weight_fraction_leaf: 0.0 n_jobs: -1 n_estimators: 100 oob_score: false verbose: 0 warm_start: false hyper_parameter_optimization: enable: true algorithm: tpe max_evals: 100 show_progressbar: true space: model_create: max_depth: - uniformint - 2 - 30 max_features: - uniform - 0.3 - 1.0 min_samples_split: - uniformint - 2 - 20 min_samples_leaf: - uniformint - 1 - 100 n_estimators: - uniformint - 2 - 1000 model: model_class: sklearn.ensemble.RandomForestRegressor model_type: regression","title":"Workflow: Model Search"},{"location":"model_search/#model-search","text":"LogMl can perform model search, that is, fit many different models and rank them according to their (validation) performance. Models used in model search (these models are based on SciKit-Learn): sklearn.dummy.DummyClassifier sklearn.dummy.DummyRegressor sklearn.ensemble.AdaBoostClassifier sklearn.ensemble.AdaBoostRegressor sklearn.ensemble.BaggingClassifier sklearn.ensemble.BaggingRegressor sklearn.ensemble.ExtraTreesClassifier sklearn.ensemble.ExtraTreesRegressor sklearn.ensemble.GradientBoostingClassifier sklearn.ensemble.GradientBoostingRegressor sklearn.ensemble.HistGradientBoostingClassifier sklearn.ensemble.HistGradientBoostingRegressor sklearn.ensemble.RandomForestClassifier sklearn.ensemble.RandomForestRegressor sklearn.linear_model.ARDRegression sklearn.linear_model.BayesianRidge sklearn.linear_model.HuberRegressor sklearn.linear_model.Lars sklearn.linear_model.Lasso sklearn.linear_model.LassoCV sklearn.linear_model.LinearRegression sklearn.linear_model.LogisticRegressionCV sklearn.linear_model.OrthogonalMatchingPursuit sklearn.linear_model.PassiveAggressiveClassifier sklearn.linear_model.Perceptron sklearn.linear_model.RANSACRegressor sklearn.linear_model.Ridge sklearn.linear_model.RidgeCV sklearn.linear_model.TheilSenRegressor sklearn.naive_bayes.BernoulliNB sklearn.naive_bayes.ComplementNB sklearn.naive_bayes.GaussianNB sklearn.naive_bayes.MultinomialNB sklearn.neighbors.KNeighborsClassifier sklearn.neighbors.KNeighborsRegressor sklearn.neighbors.NearestCentroid sklearn.neighbors.RadiusNeighborsClassifier sklearn.neighbors.RadiusNeighborsRegressor sklearn.svm.LinearSVC sklearn.svm.LinearSVR sklearn.svm.NuSVC sklearn.svm.NuSVR sklearn.svm.SVC sklearn.svm.SVR sklearn.tree.DecisionTreeClassifier sklearn.tree.DecisionTreeRegressor Note that Dummy models are included. Dummy models are useful as a simple baseline to compare with other (real) models.","title":"Model Search"},{"location":"model_search/#model-search-with-hyper-parameter-optimization","text":"Hyper-parameter optimization can be used in conjunction with model search for find a suitable set of hyper-parameters for each model. These are the models currently using hyper-parameter optimization in model search. sklearn.ensemble.ExtraTreesClassifier sklearn.ensemble.ExtraTreesRegressor sklearn.ensemble.GradientBoostingClassifier sklearn.ensemble.GradientBoostingRegressor sklearn.neighbors.KNeighborsClassifier sklearn.neighbors.KNeighborsRegressor sklearn.ensemble.RandomForestClassifier sklearn.ensemble.RandomForestRegressor","title":"Model Search with Hyper-parameter optimization:"},{"location":"model_search/#results","text":"Model search results are shown in a table, ranked by validation score. The table also shows: - train loss - validation loss - training time - all hyper-parameters used to create the model (NA if the hyper-parameters does not apply to a specific model)","title":"Results"},{"location":"model_search/#config_yaml","text":"The config_YAML pard that refers to model search simply includes all model file YAMLs in a sub directory: model_search: enable: true models: !include models_search/*.yaml Each YAML file in the sub-directory models_search is a configuration for a specific model. For instance, here is the configuration YAML for RandomForestRegressor, you can see how default parameter values and hyper parameter optimization: RandomForestRegressor: functions: model_create: bootstrap: true criterion: mse max_features: auto min_impurity_decrease: 0.0 min_samples_leaf: 1 min_samples_split: 2 min_weight_fraction_leaf: 0.0 n_jobs: -1 n_estimators: 100 oob_score: false verbose: 0 warm_start: false hyper_parameter_optimization: enable: true algorithm: tpe max_evals: 100 show_progressbar: true space: model_create: max_depth: - uniformint - 2 - 30 max_features: - uniform - 0.3 - 1.0 min_samples_split: - uniformint - 2 - 20 min_samples_leaf: - uniformint - 1 - 100 n_estimators: - uniformint - 2 - 1000 model: model_class: sklearn.ensemble.RandomForestRegressor model_type: regression","title":"Config_YAML"},{"location":"overview/","text":"Overview Log(ML) is a Machine Learning / AI automation framework. Here is a summary of what LogMl does: Example of LogMl workflow: Dataset: Load: Load a dataset from a file Transform: sanitize input names, encode cathegorical inputs, one-hot encoding, expand date/time fieds, remove duplicated inputs, remove samples with missing outputs, shuffle samples, remove inputs with low variance, impute missing values, add missing value indicator variables, normalize inputs, etc. Augment: add pricipal components (PCA), NMF, interaction variables, etc. Split: Split data into train / validate / test datasets Inputs/Outputs: Obtain inputs and output for each (train / validate / test) dataset. Explore: show dataset, correlation analysis, dendogram, missingness analysis, normality/log-normality analysis, inputs distributions, pair-plots, heatmaps, etc. Feature importance: Model-based (Random Forest, Extra trees, Gradient Boosting) by input shuffle / drop column algorithms Boruta Regularization methods (Lasso, Ridge, Lars) Recursive Feature elimination (Lasso, Ridge, Lars AIC, Lars BIC, Random Forest, Extra trees, Gradient Boosting) Linear model p-value Logistic model p-value (Wilks) Multi-class Logistic model p-value (Wilks) Summary by weighted consensus of all the previous method using model's validation score. Model Training: Single model: Train a model, calculate validation and test dataset performance Hyper-parameter optimization: Train models using hyper-parameter optimization using Bayesian or Random algorithms Model Search: Search over 50 different model families (i.e. machine learning algorithms). This search can be done using hyper-parameter optimizations on each model's hyper-parameters. Cross-validation: Perform cross-validation (KFold, RepeatedKFold, LeaveOneOut, LeavePOut, ShuffleSplit). Cross-validation can be used in all model search and Feature importance steps. Logging: All outputs are directed to log files Save models & datasets: Automatically save models and datasets for easier retrieval later","title":"Overview"},{"location":"overview/#overview","text":"Log(ML) is a Machine Learning / AI automation framework. Here is a summary of what LogMl does: Example of LogMl workflow: Dataset: Load: Load a dataset from a file Transform: sanitize input names, encode cathegorical inputs, one-hot encoding, expand date/time fieds, remove duplicated inputs, remove samples with missing outputs, shuffle samples, remove inputs with low variance, impute missing values, add missing value indicator variables, normalize inputs, etc. Augment: add pricipal components (PCA), NMF, interaction variables, etc. Split: Split data into train / validate / test datasets Inputs/Outputs: Obtain inputs and output for each (train / validate / test) dataset. Explore: show dataset, correlation analysis, dendogram, missingness analysis, normality/log-normality analysis, inputs distributions, pair-plots, heatmaps, etc. Feature importance: Model-based (Random Forest, Extra trees, Gradient Boosting) by input shuffle / drop column algorithms Boruta Regularization methods (Lasso, Ridge, Lars) Recursive Feature elimination (Lasso, Ridge, Lars AIC, Lars BIC, Random Forest, Extra trees, Gradient Boosting) Linear model p-value Logistic model p-value (Wilks) Multi-class Logistic model p-value (Wilks) Summary by weighted consensus of all the previous method using model's validation score. Model Training: Single model: Train a model, calculate validation and test dataset performance Hyper-parameter optimization: Train models using hyper-parameter optimization using Bayesian or Random algorithms Model Search: Search over 50 different model families (i.e. machine learning algorithms). This search can be done using hyper-parameter optimizations on each model's hyper-parameters. Cross-validation: Perform cross-validation (KFold, RepeatedKFold, LeaveOneOut, LeavePOut, ShuffleSplit). Cross-validation can be used in all model search and Feature importance steps. Logging: All outputs are directed to log files Save models & datasets: Automatically save models and datasets for easier retrieval later","title":"Overview"},{"location":"workflow/","text":"LogMl Workflow LogMl workflow is shown in the diagram Note that some steps have a name to identify them, these steps can be customized with user_defined_functions Dataset: dataset_load : Load a dataset from a file dataset_create : Create a dataset using a user_defined_function dataset_preprocess : Pre-process data, e.g. to make it suitable for model training. dataset_augment : Data augmentation dataset_split : Split data into train / validate / test datasets dataset_inout : Obtain inputs and output for each (train / validate / test) dataset. dataset_save : Save datasets to a file Explore: E.g. show dataset, correlation analysis, dendogram, missingness analysis, etc. Feature importance: Model: model_create : Create a new model model_train : Train a model Hyper-parameter optimization: Train models using hyper-parameter optimization using Bayesian or Random algorithms Model Search: Search over 50 different model families (i.e. machine learning algorithms). This search can be done using hyper-parameter optimizations on each model's hyper-parameters. model_evaluate : Evaluate the model using train, validation or test datasets model_predict : Generate model predictions model_save : Save model to a file Cross-validation: Perform cross-validation (KFold, RepeatedKFold, LeaveOneOut, LeavePOut, ShuffleSplit). Cross-validation can be used in all model search and Feature importance steps. Logging: All outputs are directed to log files Save models & datasets: Automatically save models and datasets for easier retrieval later Nomenclature Note that these nomenclature items are separated by underscores instead of spaces, to denote a special meaning. LogMl_workflow : The workflow the LogMl, specified in the previous section Workflow_step : Each of steps in the LogMl_workflow Named_workflow_step : A workflow_step that has a name (see previous section), e.g. The steps identified with dataset_load , dataset_preprocess , model_train , etc. workflow_step_name : The name of a Named_workflow_step , e.g. dataset_load , dataset_preprocess , model_train , etc. User_defined_function : Python code that has been annotated with a LogMl name (from a named_workflow_step ). LogMl_default_function : A function provided by LogMl, if no user_defined_function exists, the LogMl_default_function will be invoked instead Config_YAML : A LogMl configuration YAML file. The YAML file can also store parameters that are passed to user_defined_functions Soft_fail : When a workflow_step fails, warning might be shown, but LogMl_workflow execution continues Hard_fail : When a workflow_step fails, an error message is shown and LogMl_workflow is stopped Processed_dataset_file : LogMl saves datasets to (pickle) file after dataset steps (pre-processed, augmented, split), so that next time it ca be retrieved from the file instead of spending time on running the steps again. Customizing workflow steps There are two ways to customize the workflow steps: User_defined_functions : Custom (Python) code. This can be done on named_workflow_steps , such as dataset_load , dataset_preprocess , model_train , etc. Config_YAML : Configuration parameters are stored in a YAML file. How each LogMl named_workflow_step works: The user can provide a user_defined_function (custom Python code) for each named_workflow_steps . E.g. For the dataset_load workflow step, a user provides a function to load the dataset If there is no user_defined_function, a LogMl_default_function is used. E.g. For the dataset_load workflow step, LogMl tries to load a dataset from a CSV file If there is neither a user_defined_function , nor a LogMl_default_function , the step fails (can be soft_fail or hard_fail , depending on the step) User_defined_function A user_defined_function is just Python code that has been annotated with a LogMl name (from a named_workflow_step ). The user_defined_functions are often referred by the names with an @ prefix, for instance @dataset_load refers to the user_defined_function annotated with dataset_load workflow_step_name Sample code using user_defined_function to create a dataset: #!/usr/bin/env python3 import numpy as np from logml import * # This is a user_defined_function (it is annotated with a LogMl 'named_workflow_step') @dataset_create def my_dataset_create(num_samples): x = 2 * np.random.rand(num_samples, 2) - 1 y = ((x[:, 0] > 0) ^ (x[:, 1] > 0)).astype('float').reshape(num_samples, 1) return np.concatenate((x, y), axis=1) # Main lm = LogMl() # Create LogMl object lm() # Invoke it to execute the LogMl workflow (LogMl objects are callable) Config_YAML The user_defined_function we saw in the previous section ( my_dataset_create ) has a parameter num_samples . We can define the parameter in the config_YAML file: dataset: dataset_name: 'example_01' dataset_path: 'data/example_01' model: enable: True model_name: 'example_01' model_path: 'data/example_01/model' functions: dataset_create: # Parameters in this sub-section are passed num_samples: 1000 # to @dataset_create user_defined_function Note that the functions section in the config_YAML file, has a dataset_create sub-section. All parameters defined in that sub-section are passed as named parameters to the Python my_dataset_create function (i.e the user_defined_function annotated with dataset_create ). Given the code and YAML shown above, when LogMl executes the wokflow and reaches the dataset_create workflow_step , it will invoke my_dataset_create(1000) and store the return value as the dataset. The dataset will then be used in all the next workflow_steps : it will be pre-processed, split into train/validation/test, used for model training, etc. LogMl_default_functions : Creating custom functions is very flexible, but many times LogMl already has good default functions you can use. Particularly, when using a DataFrame (e.g. a dataset provided as CSV file), there are many functionalities that LogMl provides by default. These LogMl_default_functions have many parameters that are defined in the config_YAML","title":"Workflow"},{"location":"workflow/#logml-workflow","text":"LogMl workflow is shown in the diagram Note that some steps have a name to identify them, these steps can be customized with user_defined_functions Dataset: dataset_load : Load a dataset from a file dataset_create : Create a dataset using a user_defined_function dataset_preprocess : Pre-process data, e.g. to make it suitable for model training. dataset_augment : Data augmentation dataset_split : Split data into train / validate / test datasets dataset_inout : Obtain inputs and output for each (train / validate / test) dataset. dataset_save : Save datasets to a file Explore: E.g. show dataset, correlation analysis, dendogram, missingness analysis, etc. Feature importance: Model: model_create : Create a new model model_train : Train a model Hyper-parameter optimization: Train models using hyper-parameter optimization using Bayesian or Random algorithms Model Search: Search over 50 different model families (i.e. machine learning algorithms). This search can be done using hyper-parameter optimizations on each model's hyper-parameters. model_evaluate : Evaluate the model using train, validation or test datasets model_predict : Generate model predictions model_save : Save model to a file Cross-validation: Perform cross-validation (KFold, RepeatedKFold, LeaveOneOut, LeavePOut, ShuffleSplit). Cross-validation can be used in all model search and Feature importance steps. Logging: All outputs are directed to log files Save models & datasets: Automatically save models and datasets for easier retrieval later","title":"LogMl Workflow"},{"location":"workflow/#nomenclature","text":"Note that these nomenclature items are separated by underscores instead of spaces, to denote a special meaning. LogMl_workflow : The workflow the LogMl, specified in the previous section Workflow_step : Each of steps in the LogMl_workflow Named_workflow_step : A workflow_step that has a name (see previous section), e.g. The steps identified with dataset_load , dataset_preprocess , model_train , etc. workflow_step_name : The name of a Named_workflow_step , e.g. dataset_load , dataset_preprocess , model_train , etc. User_defined_function : Python code that has been annotated with a LogMl name (from a named_workflow_step ). LogMl_default_function : A function provided by LogMl, if no user_defined_function exists, the LogMl_default_function will be invoked instead Config_YAML : A LogMl configuration YAML file. The YAML file can also store parameters that are passed to user_defined_functions Soft_fail : When a workflow_step fails, warning might be shown, but LogMl_workflow execution continues Hard_fail : When a workflow_step fails, an error message is shown and LogMl_workflow is stopped Processed_dataset_file : LogMl saves datasets to (pickle) file after dataset steps (pre-processed, augmented, split), so that next time it ca be retrieved from the file instead of spending time on running the steps again.","title":"Nomenclature"},{"location":"workflow/#customizing-workflow-steps","text":"There are two ways to customize the workflow steps: User_defined_functions : Custom (Python) code. This can be done on named_workflow_steps , such as dataset_load , dataset_preprocess , model_train , etc. Config_YAML : Configuration parameters are stored in a YAML file. How each LogMl named_workflow_step works: The user can provide a user_defined_function (custom Python code) for each named_workflow_steps . E.g. For the dataset_load workflow step, a user provides a function to load the dataset If there is no user_defined_function, a LogMl_default_function is used. E.g. For the dataset_load workflow step, LogMl tries to load a dataset from a CSV file If there is neither a user_defined_function , nor a LogMl_default_function , the step fails (can be soft_fail or hard_fail , depending on the step)","title":"Customizing workflow steps"},{"location":"workflow/#user_defined_function","text":"A user_defined_function is just Python code that has been annotated with a LogMl name (from a named_workflow_step ). The user_defined_functions are often referred by the names with an @ prefix, for instance @dataset_load refers to the user_defined_function annotated with dataset_load workflow_step_name Sample code using user_defined_function to create a dataset: #!/usr/bin/env python3 import numpy as np from logml import * # This is a user_defined_function (it is annotated with a LogMl 'named_workflow_step') @dataset_create def my_dataset_create(num_samples): x = 2 * np.random.rand(num_samples, 2) - 1 y = ((x[:, 0] > 0) ^ (x[:, 1] > 0)).astype('float').reshape(num_samples, 1) return np.concatenate((x, y), axis=1) # Main lm = LogMl() # Create LogMl object lm() # Invoke it to execute the LogMl workflow (LogMl objects are callable)","title":"User_defined_function"},{"location":"workflow/#config_yaml","text":"The user_defined_function we saw in the previous section ( my_dataset_create ) has a parameter num_samples . We can define the parameter in the config_YAML file: dataset: dataset_name: 'example_01' dataset_path: 'data/example_01' model: enable: True model_name: 'example_01' model_path: 'data/example_01/model' functions: dataset_create: # Parameters in this sub-section are passed num_samples: 1000 # to @dataset_create user_defined_function Note that the functions section in the config_YAML file, has a dataset_create sub-section. All parameters defined in that sub-section are passed as named parameters to the Python my_dataset_create function (i.e the user_defined_function annotated with dataset_create ). Given the code and YAML shown above, when LogMl executes the wokflow and reaches the dataset_create workflow_step , it will invoke my_dataset_create(1000) and store the return value as the dataset. The dataset will then be used in all the next workflow_steps : it will be pre-processed, split into train/validation/test, used for model training, etc. LogMl_default_functions : Creating custom functions is very flexible, but many times LogMl already has good default functions you can use. Particularly, when using a DataFrame (e.g. a dataset provided as CSV file), there are many functionalities that LogMl provides by default. These LogMl_default_functions have many parameters that are defined in the config_YAML","title":"Config_YAML"}]}