<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Datasets - LogMl</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Datasets";
    var mkdocs_page_input_path = "datasets.md";
    var mkdocs_page_url = "/datasets/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> LogMl</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">LogMl</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../about/">About</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../cross_validation/">Cross-validation</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Datasets</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#datasets">Datasets</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#config_yaml">Config_YAML</a></li>
        
            <li><a class="toctree-l3" href="#dataset-load">Dataset: Load</a></li>
        
            <li><a class="toctree-l3" href="#dataset-create">Dataset: Create</a></li>
        
            <li><a class="toctree-l3" href="#dataset-preprocess">Dataset: Preprocess</a></li>
        
            <li><a class="toctree-l3" href="#dataset-augment">Dataset: Augment</a></li>
        
            <li><a class="toctree-l3" href="#dataset-split">Dataset: Split</a></li>
        
            <li><a class="toctree-l3" href="#dataset-inputs-outputs">Dataset: Inputs / Outputs</a></li>
        
            <li><a class="toctree-l3" href="#dataset-save">Dataset: Save</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#dataframes">DataFrames</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#dataset-df-default-load">Dataset (df): Default Load</a></li>
        
            <li><a class="toctree-l3" href="#dataset-df-default-preprocess">Dataset (df): Default Preprocess</a></li>
        
            <li><a class="toctree-l3" href="#dataset-df-default-augment">Dataset (df): Default Augment</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../examples_custom_fucntions/">Examples custom fucntions</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../explore/">Explore</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../feature_importance/">Feature importance</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../install/">Install</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../introduction/">Introduction</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../introduction_cmd/">Introduction cmd</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../introduction_juppyter/">Introduction juppyter</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../logging/">Logging</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../model/">Model</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../model_hyperopt/">Model hyperopt</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../model_search/">Model search</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../overview/">Overview</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../workflow/">Workflow</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">LogMl</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Datasets</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="datasets">Datasets</h1>
<p>The first steps in a <em>LogMl_workflow</em> perform dataset operations.
There are several <em>named_workflow_steps</em> taking care of different aspects of dataset processing to make sure is suitable for training a model</p>
<ol>
<li><code>dataset_load</code>: Load a dataset from a file</li>
<li><code>dataset_create</code>: Create a dataset using a user_defined_function</li>
<li><code>dataset_preprocess</code>: Pre-process data, e.g. to make it suitable for model training.</li>
<li><code>dataset_augment</code>: Data augmentation</li>
<li><code>dataset_split</code>: Split data into train / validate / test datasets</li>
<li><code>dataset_inout</code>: Obtain inputs and output for each (train / validate / test) dataset.</li>
<li><code>dataset_save</code>: Save datasets to a file</li>
</ol>
<h3 id="config_yaml">Config_YAML</h3>
<p>There are several parameters that can be defined in the <em>config_YAML</em>:</p>
<pre><code>dataset:
  # Dataset type: 'df' means dataFrame
  dataset_type: 'df'

  # Dataset name:
  #   A simple name for the dataset.
  #   It is appended to dataset_path to create dataset_file_name
  dataset_name: 'my_dataset'

  # Dataset Path: Path to use when loading and saving datasets
  dataset_path: 'data/my_dataset'

  # If set, loading dataset from pickle file will be skipped
  do_not_load_pickle: false

  # If set, saving dataset will be skipped
  do_not_save: false

  # Use all inputs, no outputs.
  # I.e. do not split in/out.
  # E.g. unsupervised learning
  is_use_all_inputs: false

  # Use default 'in_out' method
  is_use_default_in_out: true

  # Use default preprocess
  is_use_default_preprocess: true

  # Use (internal) 'split' function if none is provided by the user?
  # Note: This function returns dataset as a list
  is_use_default_split: false

  # Use default transform operation
  is_use_default_transform: true

  # Output variables, a.k.a. dependent variables (i.e. what we want to predict)
  outputs: []
</code></pre>

<h3 id="dataset-load">Dataset: Load</h3>
<p>This step typical attempts to load data from files (e.g. load a "data frame" or a set of images).</p>
<ol>
<li>Attempt to load from pickle file (<code>{dataset_path}/{dataset_name}.pkl</code>). If the files exists, load the dataset.</li>
<li>Invoke a <em>user_defined_function</em> decorated with <code>@dataset_load</code>.<ul>
<li>If there is no function or the section is disabled in the config file (i.e. <code>enable=False</code>), this step has failed, so LogMl will attempt to create a dataset (next step)</li>
<li>Parameters to the <em>user_defined_function</em> are defined in <em>config_YAML</em> file section <code>functions</code>, sub-section <code>dataset_load</code></li>
<li>The dataset is marked to be saved</li>
</ul>
</li>
</ol>
<h3 id="dataset-create">Dataset: Create</h3>
<p>This part creates a dataset by invoking the <em>user_defined_function</em> decorated by <code>@dataset_create</code></p>
<ol>
<li>If the dataset has been loaded (i.e. "Load dataset" step was successful), skip this step</li>
<li>Invoke a <em>user_defined_function</em> decorated with <code>@dataset_create</code>:<ul>
<li>If there is no <em>user_defined_function</em> or the section is disabled in the config file (i.e. <code>enable=False</code>), this step <em>hard_fails</em>, i.e. since LogMl doesn't have a dataset to work with (load and create steps both failed) it will exit with an error.</li>
<li>Parameters to the <em>user_defined_function</em> are defined in <em>config_YAML</em> file section <code>functions</code>, sub-section <code>dataset_create</code></li>
<li>The return value from the <em>user_defined_function</em> is used as a dataset</li>
<li>The dataset is marked to be saved</li>
</ul>
</li>
</ol>
<h3 id="dataset-preprocess">Dataset: Preprocess</h3>
<p>This step is used to pre-process data in order to make the dataset compatible with the inputs required by the model (e.g. normalize values)</p>
<ol>
<li>If this step has already been performed (i.e. a dataset loaded from a pickle file that has already been pre-processed), the step is skipped</li>
<li>Invoke a <em>user_defined_function</em> decorated with <code>@dataset_preprocess</code>:<ul>
<li>If there is no function or the section is disabled in the config file (i.e. <code>enable=False</code>), this step has failed, no error is produced.</li>
<li>Parameters: The first parameter is the dataset. Other parameters are defined in <em>config_YAML</em> file section <code>functions</code>, sub-section <code>dataset_preprocess</code></li>
<li>The return value replaces the original dataset</li>
<li>The dataset is marked to be saved</li>
</ul>
</li>
</ol>
<h3 id="dataset-augment">Dataset: Augment</h3>
<p>This step invokes the <em>user_defined_function</em> <code>@augment</code> to perform dataset augmentation</p>
<ol>
<li>If this step has already been performed (i.e. a dataset loaded from a pickle file that has already been augmented), the step is skipped</li>
<li>Invoke a <em>user_defined_function</em> decorated with <code>@dataset_augment</code>:<ul>
<li>If there is no function or the section is disabled in the config file (i.e. <code>enable=False</code>), this step has failed, no error is produced.</li>
<li>Parameters: The first parameter is the dataset. Other parameters are defined in <em>config_YAML</em> file section <code>functions</code>, sub-section <code>dataset_augment</code></li>
<li>The return value replaces the original dataset</li>
<li>The dataset is marked to be saved</li>
</ul>
</li>
</ol>
<h3 id="dataset-split">Dataset: Split</h3>
<p>This step us used to split the dataset into "train", "test" and "validation" datasets.</p>
<ol>
<li>If this step has already been performed (i.e. a dataset loaded from a pickle file that has already been split), the step is skipped</li>
<li>Invoke a <em>user_defined_function</em> decorated with <code>@dataset_split</code>:<ol>
<li>If there is no <em>user_defined_function</em> <code>@dataset_split</code> or the section is disabled in the config file (i.e. <code>enable=False</code>), this step has failed (no error is produced) attempt to use a <em>LogMl_default_function</em> of <code>dataset_split</code>.</li>
<li>Parameters: The first parameter is the dataset. Other parameters are defined in <em>config_YAML</em> file section <code>functions</code>, sub-section <code>dataset_split</code></li>
<li>If the function is invoked, the return value must be a tuple of three datasets: <code>(dataset_train, dataset_test, dataset_validate)</code></li>
<li>The return value from the function replaces the original dataset (specifically, each value replaces the train/test/validate datasets)</li>
<li>The dataset is marked to be saved</li>
</ol>
</li>
<li>Attempt to use a <em>LogMl_default_function</em> of <code>dataset_split</code><ul>
<li>If <em>config_YAML</em> parameter <code>is_use_default_split</code> is set to <code>False</code>, the split step failed (no error is produced)</li>
<li>The default split implementation attempts to split the dataset in three parts, defined by <em>config_YAML</em> file parameters <code>split_test</code> and <code>split_validate</code> in section <code>dataset_split</code>. If these parameters are not defined in the <em>config_YAML</em> file, the split section failed (no error is produced)</li>
</ul>
</li>
</ol>
<h3 id="dataset-inputs-outputs">Dataset: Inputs / Outputs</h3>
<ol>
<li>For each dataset splie (train, validation, and test):<ol>
<li>Invoke a <em>user_defined_function</em> decorated with <code>@dataset_inout</code>:<ol>
<li>If there is no <em>user_defined_function</em> <code>@dataset_inout</code> or the section is disabled in the config file (i.e. <code>enable=False</code>), this step fails (no error is produced) and LogMl attempts to use a <em>LogMl_default_function</em> for <code>dataset_inout</code>.</li>
<li>Parameters: The first parameter is the dataset, the second parameter is the datset name (<code>train</code>, <code>validate</code>, or <code>test</code>). Other parameters are defined in <em>config_YAML</em> file section <code>functions</code>, sub-section <code>dataset_inout</code></li>
<li>If the function is invoked, the return value must be a tuple: <code>(inputs, outputs)</code></li>
<li>The return value from the function replaces the original dataset (specifically, each value replaces the train/test/validate datasets)</li>
<li>The dataset is marked to be saved</li>
</ol>
</li>
<li>Attempt to use a <em>LogMl_default_function</em> of <code>dataset_inout</code><ul>
<li>If <em>config_YAML</em> parameter <code>is_use_default_in_out</code> is set to <code>False</code>, the split step <em>hard_fails</em> (execution is halted)</li>
<li>The default implementation only returns the same dataset is the <em>config_YAML</em> parameter <code>is_use_all_inputs</code> is set. Otherwise it <em>hard_fails</em>.</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="dataset-save">Dataset: Save</h3>
<p>If the dataset is marked to be saved in any of the previous steps, attempt to save the dataset.</p>
<ol>
<li>If the the YAML config variable <code>do_not_save</code> is set to <code>True</code>, this step is skipped</li>
<li>If a <em>user_defined_function</em> decorated with <code>@dataset_save</code> exists, it is invoked<ul>
<li>Parameters: The first four parameters are <code>dataset, dataset_train, dataset_test, dataset_validate</code>. Other parameters are defined in <em>config_YAML</em> file section <code>functions</code>, sub-section <code>dataset_save</code></li>
</ul>
</li>
<li>Otherwise the dataset is saved to the pickle file <code>{dataset_path}/{dataset_name}.pkl</code></li>
</ol>
<h1 id="dataframes">DataFrames</h1>
<p>There are several useful <em>LogMl_default_function</em> then the dataset is a DataFrame, i.e. when <em>config_YAML</em> defines <code>dataset_type='df'</code>.</p>
<h3 id="dataset-df-default-load">Dataset (df): Default Load</h3>
<p>Load DataFrame from a CSV file using (using <code>pandas.read_csv</code>).</p>
<h3 id="dataset-df-default-preprocess">Dataset (df): Default Preprocess</h3>
<p>The default method for DataFrame pre-processing does:</p>
<ol>
<li>Sanitize variables names: Convert names so that characters outside the set <code>[_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789]</code> are converted to <code>_</code> (underscore)</li>
<li>Convert to categorical: Fields defined in the <em>config_YAML</em> file, section <code>dataset</code>, sub-section <code>categories</code> are converted into categorical data and converted to a numerical (integer) representation. Category <code>0</code> represents missing values.</li>
<li>Convert to one-hot: Fields defined in the <em>config_YAML</em> file, section <code>dataset</code>, sub-section <code>ont_hot</code> are converted into one-hot encoding. Also, any categorical field that has a cardinality (i.e. number of categories) equal or less then <code>one_hot_max_cardinality</code> is converted to one-hot encoding. If there are missing values, a column <code>*_isna</code> is added to the one-hot encoding.</li>
<li>Missing data indicators: In any column having missing values that was not converted to date, categorical or one-hot; a new column <code>*_na</code> is created (where the value is '1' if the field has missing a value) and the missing values are replaced by the median of the non-missing values.</li>
<li>Expand date/time features: Fields defined in the <em>config_YAML</em> file, section <code>dataset</code>, sub-section <code>dates</code> are treated as date/time when the dataFrame CSV is loaded and then expanded into several columns: <code>[Year, Month, Day, DayOfWeek, Hour, Minute, Second, etc.]</code>.</li>
<li>Remove samples with missing outputs: Can be disables using <em>config_YAML</em> option <code>remove_missing_outputs</code></li>
<li>Drop low standard deviation fields: All fields having standard deviation equal or lower than <code>std_threshold</code> (by default <code>0.0</code>) are dropped. Using the default value (<code>std_threshold=0.0</code>) this means dropping all fields having the exact same value for all samples.</li>
<li>Remove duplicated inputs (e.g. two columns that are exactly the same)</li>
<li>Shuffle samples: Can be enabled using <em>config_YAML</em> option <code>shuffle</code></li>
<li>Balance dataset: re-sample categories having low number of samples. Can be enabled using <em>config_YAML</em> option <code>balance</code></li>
<li>Impute missing values. Can use several strategies: <code>[mean, median, most_frequent, one, zero]</code></li>
<li>Normalize variables: Can use several strategies: <code>[standard, maxabs, minmax, minmax_neg, log, log1p, log_standard, log1p_standard, quantile]</code></li>
</ol>
<p><strong>Config_YAML Options</strong> for datasets (df) pre-process</p>
<pre><code>dataset_preprocess:
  # Set to 'false' to disable this step
  enable: true

  # Categorical data
  # A list of categorical input variables
  # Can be followed by a list of categories to enforce some type of categorical order in the codes
  categories:
    UsageBand: ['Low', 'Medium', 'High']
    ProductSize: ['Mini', 'Small', 'Compact', 'Medium', 'Large / Medium', 'Large']

  # Categorical data: Match field names using regular expressions
  # A list of categorical input variables
  # Can be followed by a list of categories to enforce some type of categorical order in the codes
  categories_regex:
    - 'zzz_.*': ['low', 'mid', 'high']
    - 'yyy_.*': True

  # List of data columns that are transformed into 'date/time'
  # These columns are also split into 'date parts' (year, month, day, day_of_week, etc.)
  dates: ['saledate']

  # Sanitize column names: Convert column names so that characters outside the set [_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789] are converted to '_' (underscore)
  is_sanitize_column_names: True

  # One hot encoding: List of variable to transform to one_hot
  ont_hot: ['Enclosure_Type']

  # One hot encoding: Encode all variables having a cardinality less or equal to 'one_hot_max_cardinality'
  one_hot_max_cardinality: 7

  # Remove repeated inputs (i.e. input variables that have the exact same values as other inputs)
  remove_equal_inputs: true

  # Remove rows having missing output/s
  remove_missing_outputs: true

  # Remove columns: List of columns to remove from dataFrame
  remove_columns: []

  # Remove columns: List of columns to remove from dataFrame (after all transformations have been applied)
  remove_columns_after: []

  # Shuffle samples (e.g. shuffle rows in dataframe)
  shuffle: false

  # Drop inputs having standard deviation below this limit
  std_threshold: 0.0001

  # Balance an unbalanced datasets (classification models only, since outputs must be cathegorical)
  balance: false

  # Impute values
  # Imputation method name, followed by list of variables, regex or 'true' (this last option means &quot;use as default method&quot;)
  # that all variables should be imputed using this method
  impute:
    # Use the mean value
    mean:  ['x2', 'x5', 'xx.*']

    # Impute using the median value
    median: true

    # Impute using the most frequent value
    most_frequent: ['x3', 'x4']

    # Impute by assigning value '1'
    one: ['x9', 'o.*']

    # Do not impute these variables
    skip: ['x7', 'x8']

    # Impute by assigning value '0'
    zero: ['x10', 'z.*']


  # Normalize values
  # Normalization methods followed by list of variables, regex or 'true' meaning
  # that all variables should be normalized using this method
  normalize:
    # Transform so that mean(x)=0 var(x)=1
    # Set to 'true' means to use as default (i.e. use this normalization
    # for all variables not explicitly define elsewhere)
    standard: true

    # Normalize dividing by max(abs(x_i))
    maxabs: ['x2', 'x5', 'xx.*']

    # Use min/max normalizer, i.e. transform to interval [0,1]
    minmax: ['x3']

    # Use min/max normalizer with negative values, i.e. transform to interval [-1,1]
    minmax_neg: ['x6', 'neg_.*']

    # Apply log(x)
    log: ['x9']

    # Apply log(x+1)
    log1p: ['x10', 'x11']

    # Apply log(x), then use standard transform: (log(x) - mean(log(x))) / std(log(x))
    log_standard: ['x9']

    # Apply log(x+1), then use standard transform: (log(x+1) - mean(log(x+1))) / std(log(x+1))
    log1p_standard: ['x10', 'x11']

    # Quantile transformation: This method transforms the features to follow a uniform or a normal distribution
    quantile: ['q.*']

    # Do not normalize these variables
    skip: ['x7', 'x8']
</code></pre>

<h3 id="dataset-df-default-augment">Dataset (df): Default Augment</h3>
<p>The <em>LogMl_default_function</em> for dataset augmentation focusses on adding more columns to the dataset (i.e. augment the input variables), as opposed to augmenting the number of samples.</p>
<ol>
<li>Add Principal components (PCA values): Can be customized to act on several groups of variables.</li>
<li>Add Non-negative Matrix Factorization components (NMF values): Can be customized to act on several groups of variables.</li>
</ol>
<p><strong>Config_YAML Options</strong> for datasets (df) augment:</p>
<pre><code>dataset_augment:
  # Set to 'false' to disable this step
  enable: true

  # Add principal componenets
  # After the 'pca', you can add several &quot;sub-sections&quot;:
  #   name:                 # Unique name, used to add PCA fields on each sample (name_1, name_2, ... name_num)
  #     num: 2              # Number of PCA componenets to add
  #     fields: ['x.*']     # List of regular expression to match field names (only fields matching this list will be used)
  pca:
    pca_dna:
      num: 2
      fields: ['DNA_.*']
    pca_log2:
      num: 3
      fields: ['log2ratio_.*', '.*_LOG2F']
  # Add NMF (non-negative matrix factorization)
  # After the 'nmf', you can add several &quot;sub-sections&quot;:
  #   name:                 # Unique name, used to add NMF fields on each sample (name_1, name_2, ... name_num)
  #     num: 2              # Number of NMF componenets to add
  #     fields: ['x.*']     # List of regular expression to match field names (only fields matching this list will be used)
  nmf:
    nmf_dna:
      num: 2
      fields: ['DNA_.*']
    nmf_expr:
      num: 3
      fields: ['expr_.*', '.*_EXPR']
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../examples_custom_fucntions/" class="btn btn-neutral float-right" title="Examples custom fucntions">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../cross_validation/" class="btn btn-neutral" title="Cross-validation"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../cross_validation/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../examples_custom_fucntions/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js"></script>
      <script src="../search/require.js"></script>
      <script src="../search/search.js"></script>

</body>
</html>
