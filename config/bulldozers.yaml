
#---
# LogMl parameters
#---
logger:

#---
# Dataset parameters
#---
dataset:
  # Dataset type: 'df' means dataFrame
  dataset_type: 'df'
  # Dataset name:
  #   A simple name for the dataset.
  #   It is appended to dataset_path to create dataset_file_name
  dataset_name: 'bulldozers'

  # Dataset Path: Path to use when loading and saving datasets
  dataset_path: 'data/bulldozers'

  # Use (internal) 'split' function if none is provided by the user?
  # Note: This function returns dataset as a list
  is_use_default_split: False

  # Categorical data
  categories:
    UsageBand: ['Low', 'Medium', 'High']
    ProductSize: ['Mini', 'Small', 'Compact', 'Medium', 'Large / Medium', 'Large']
  dates: ['saledate']

  # One hot encoding
  ont_hot: ['Enclosure_Type']
  one_hot_max_cardinality: 7

  # Output variables (what we want to predict)
  outputs: ['SalePrice']

  # Drop columns having standard deviation below this limit
  std_threshold: 0.0001

#---
# Dataset exploration
#---
dataset_explore:
  enable: False
  is_use_ori: True

#---
# Model definition and parameters
#---
model:
  enable: True

  # Type of model: {'clasification', 'regression', 'unsupervised'}
  model_type: regression

  # Model name: A simple string to use for file names related to this model
  model_name: 'bulldozers'

  # Buit-in model
  model_class: sklearn.ensemble.RandomForestRegressor

  # Train path: A path where to store logs and data from training
  model_path: 'data/bulldozers/model'

  # Try to save model using a pickle file?
  is_save_model_pickle: True

  # Try to save model using a 'model.save()'?
  is_save_model_method: True

  # Model file extesion
  is_save_model_method_ext: 'model'

  # Save parameters to YAML file
  is_save_params: True

  # Save model test results to a pickle file?
  is_save_test_pickle: True

  # Save model train results to pickle file?
  is_save_train_pickle: False

  # Save model validation results to pickle file?
  is_save_validate_pickle: False

#---
# Cross validation methodology
#---
cross_validation:
    enable: False
    KFold:
        n_splits: 5

#---
# Hyper-parameter optimization.
#---
hyper_parameter_optimization:
    # Set this to 'True' to enable hyper-parameter optimization
    enable: False
    # Show progress bar
    show_progressbar: True
    # Algorithm:
    #   - 'tpe': Bayesian Tree of Parzen Estimators (this should be the default)
    #   - 'random': Random search
    algorithm: 'tpe'
    # Max number of hyper-parameter evaluations. Keep in mnd that each evaluation is a full model training
    max_evals: 100
    # Parameter space to explore
    space:
        # Parameters space for 'train_model'. Names have to match exactly the ones in 'train_model' parameters section
        # The format is:
        #     parameter_name: ['distribution', distribution)parameters...]
        # For distribution names and parametres, see:
        #     https://github.com/hyperopt/hyperopt/wiki/FMin
        #     (section 'Parameter Expressions')
        model_create:
            n_estimators: ['randint', 1000]
            min_samples_leaf: ['randint', 100]
            max_features: ['uniform', 0.0, 1.0]

#---
# User defined functions
# Parameters that are passed to the respective functions.
#
# E.g. If you have the parameter 'learn_reate = 0.01234' in 'train_model'
#      section, when your train function is invokde, it will be invoked as:
#           train_model(...., learn_reate=0.01234)
#      i.e. it will have kwargs={learn_reate:0.01234}
#      These parameters will also be saved to the training-specific YAML
#      file, so you can easily keep track of them
#
# Note: To disable individual items, you can add an 'enable: False' parameter
#---
functions:
    # User defined function: Dataset augmentation
    # Parameters: First parameter is always 'dataset'
    # Returns: Augmented dataset
    dataset_augment:

    # User defined function: Create dataset
    # Returns: A dataset
    dataset_create:

    # User defined function: Load dataset
    # Returns: A dataset
    dataset_load:

    # User defined function: Save dataset
    # Parameters: First four parameters are 'dataset, dataset_train, dataset_test, dataset_validate'
    dataset_save:

    # User defined function: Split dataset
    # Parameters: First parameter is always 'dataset'
    # Returns: Dataset split into three parts: 'dataset_train, dataset_test, dataset_validate'
    dataset_split:
      n: 1000

    # User defined function: Dataset pre-processing
    # Parameters: First parameter is always 'dataset'
    # Returns: Pre-processed dataset
    dataset_preprocess:

    # User defined function: Create model
    # Parameters: First parameter is 'dataset_train' (or 'dataset' if there is no dataset_test)
    # Returns: A model to be traind on dataset_train
    model_create:
      n_estimators: 30
      min_samples_leaf: 3
      max_features: 0.7

    # User defined function: Evaluate model
    # Parameters: First parameter is dataset to evaluate (usually dataset_test or dataset_validate)
    # Returns: Results from evaliating the dataset (usually a loss)
    model_evaluate:

    # User defined function: Save model
    # Parameters: First parameter is 'model'
    model_save:

    # User defined function: Training
    # Parameters: First parameters are 'model' and dataset (usually dataset_train)
    # Returns: Results from evaliating the dataset (usually a loss)
    model_train:

#---
# Model search parameters
#---
model_search:
  enable: True

  # List of models to try
  models:
    #---
    # Models: Generalized linear regression
    #---
    # Linear regression
    LinearRegression:
        model:
          model_type: regression
          model_class: sklearn.linear_model.LinearRegression
        functions:
          model_create:
            fit_intercept: True
            normalize: True
            copy_X: True
    # Ridge regression (linear model + L2 norm penalty)
    RidgeRegression:
        model:
          model_type: regression
          model_class: sklearn.linear_model.Ridge
        functions:
          model_create:
            alpha: 1.0
            fit_intercept: True
            normalize: True
            tol: 0.001
            solver: 'auto'
    # Ridge regression with Cross-validation to search for alpha
    RidgeCVRegression:
        model:
          model_type: regression
          model_class: sklearn.linear_model.RidgeCV
        functions:
          model_create:
            # alphas: [0.1, 1.0, 10.0]
            fit_intercept: True
            normalize: True
            store_cv_values: False
    # Lasso regression (linear model + L1 norm penalty)
    LassoRegression:
        model:
          model_type: regression
          model_class: sklearn.linear_model.Lasso
        functions:
          model_create:
            alpha: 1.0
            fit_intercept: True
            normalize: True
            precompute: False
            copy_X: True
            max_iter: 1000
            tol: 0.0001
            warm_start: False
            positive: False
            selection: 'cyclic'
    # Lasso regression with alpha search using cross-validation
    LassoCVRegression:
        model:
          model_type: regression
          model_class: sklearn.linear_model.LassoCV
        functions:
          model_create:
            eps: 0.001
            n_alphas: 100
            fit_intercept: True
            normalize: True
            precompute: 'auto'
            max_iter: 1000
            tol: 0.0001
            copy_X: True
            cv: 'warn'
            verbose: False
            positive: False
            selection: 'cyclic'
    # ElasticNet: Linear regression model trained with both L1 and L2 norm regularization
    # of the coefficients. Parameter search using Cross-validation
    ElasticNetCV:
        model:
          model_type: regression
          model_class: sklearn.linear_model.OrthogonalMatchingPursuit
        functions:
          model_create:
            l1_ratio: 0.5
            eps: 0.001
            n_alphas: 100
            fit_intercept: True
            normalize: True
            precompute: 'auto'
            max_iter: 1000
            tol: 0.0001
            cv: 'warn'
            copy_X: True
            verbose: 0
            positive: False
            selection: 'cyclic'
    # LARS regression (Least-Angle Regression)
    LarsRegression:
        model:
          model_type: regression
          model_class: sklearn.linear_model.Lars
        functions:
          model_create:
            fit_intercept: True
            verbose: False
            normalize: True
            precompute: 'auto'
            n_nonzero_coefs: 500
            eps: 2.220446049250313e-16
            copy_X: True
            fit_path: True
    # Orthogonal Matching Pursuit regression
    OrthogonalMatchingPursuitRegression:
        model:
          model_type: regression
          model_class: sklearn.linear_model.OrthogonalMatchingPursuit
        functions:
          model_create:
            fit_intercept: True
            normalize: True
            precompute: 'auto'
    # Bayesian Ridge regression
    BayesianRidge:
        model:
          model_type: regression
          model_class: sklearn.linear_model.BayesianRidge
        functions:
          model_create:
            n_iter: 300
            tol: 0.001
            alpha_1: 1.0e-06
            alpha_2: 1.0e-06
            lambda_1: 1.0e-06
            lambda_2: 1.0e-06
            compute_score: False
            fit_intercept: True
            normalize: True
            copy_X: True
            verbose: False
    # Automatic Relevance Determination: Similar to Bayesian Ridge Regression, but can lead to sparser coefficients
    ARDRegression:
        model:
          model_type: regression
          model_class: sklearn.linear_model.ARDRegression
        functions:
          model_create:
            n_iter: 300
            tol: 0.001
            alpha_1: 1.0e-06
            alpha_2: 1.0e-06
            lambda_1: 1.0e-06
            lambda_2: 1.0e-06
            compute_score: False
            threshold_lambda: 10000.0
            fit_intercept: True
            normalize: True
            copy_X: True
            verbose: False
    # RANSAC (RANdom SAmple Consensus) fits a model from random subsets of inliers from the complete data set
    # RANSACRegressor:
        model:
          model_type: regression
          model_class: sklearn.linear_model.RANSACRegressor
        functions:
          model_create:
            max_trials: 100
            max_skips: .inf
            stop_n_inliers: .inf
            stop_score: .inf
            stop_probability: 0.99
            loss: 'absolute_loss'
    # TheilSenRegressor estimator uses a generalization of the median in multiple dimensions
    TheilSenRegressor:
        model:
          model_type: regression
          model_class: sklearn.linear_model.TheilSenRegressor
        functions:
          model_create:
            fit_intercept: True
            copy_X: True
            max_subpopulation: 10000.0
            max_iter: 300
            tol: 0.001
            verbose: False
    # HuberRegressor is "robust" Ridge because it applies a linear loss to samples that are classified as outliers
    HuberRegressor:
        model:
          model_type: regression
          model_class: sklearn.linear_model.HuberRegressor
        functions:
          model_create:
            epsilon: 1.35
            max_iter: 100
            alpha: 0.0001
            warm_start: False
            fit_intercept: True
            tol: 1e-05
    #---
    # Models: Generalized linear classifiers
    #---
    # Logistic regression, despite its name, is a linear model for classification rather than regression
    LogisticRegressionCV:
        model:
          model_type: classification
          model_class: sklearn.linear_model.LogisticRegressionCV
        functions:
          model_create:
            Cs: 10
            fit_intercept: True
            cv: 'warn'
            dual: False
            penalty: 'l2'
            solver: 'lbfgs'
            tol: 0.0001
            max_iter: 100
            verbose: 0
            refit: True
            intercept_scaling: 1.0
            multi_class: 'warn'
    # Perceptron is another simple classification algorithm suitable for large scale learning
    Perceptron:
        model:
          model_type: classification
          model_class: sklearn.linear_model.Perceptron
        functions:
          model_create:
            alpha: 0.0001
            fit_intercept: True
            max_iter: 1000
            tol: 0.001
            shuffle: True
            verbose: 0
            eta0: 1.0
            random_state: 0
            early_stopping: False
            validation_fraction: 0.1
            n_iter_no_change: 5
            warm_start: False
    # The passive-aggressive algorithms are a family of algorithms for large-scale
    # learning. They are similar to the Perceptron in that they do not require a
    # learning rate
    PassiveAggressiveClassifier:
        model:
          model_type: classification
          model_class: sklearn.linear_model.PassiveAggressiveClassifier
        functions:
          model_create:
            C: 1.0
            fit_intercept: True
            max_iter: 1000
            tol: 0.001
            early_stopping: False
            validation_fraction: 0.1
            n_iter_no_change: 5
            shuffle: True
            verbose: 0
            loss: 'hinge'
            warm_start: False
            average: False
    #---
    # Models: Regression
    #---
    # Rnadom forest: An ensamble of random trees
    RandomForestRegressor:
        model:
          model_type: regression
          model_class: sklearn.ensemble.RandomForestRegressor
        functions:
          model_create:
            n_estimators: 100
            criterion: 'mse'
            # max_depth: None
            min_samples_split: 2
            min_samples_leaf: 1
            min_weight_fraction_leaf: 0.0
            max_features: 'auto'
            # max_leaf_nodes: None
            min_impurity_decrease: 0.0
            # min_impurity_split: None
            bootstrap: True
            oob_score: False
            # n_jobs: None
            # random_state: None
            verbose: 0
            warm_start: False
        hyper_parameter_optimization:
          enable: False
          show_progressbar: True
          algorithm: 'tpe'
          max_evals: 100
          space:
              model_create:
                  n_estimators: ['randint', 1000]
                  min_samples_leaf: ['randint', 100]
                  max_features: ['uniform', 0.3, 1.0]
