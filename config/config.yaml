
#---
# LogMl model config file
#---

#---
# LogMl parameters
#---
logger:
  # Plots configuration
  disable_plots: false      # If true, no plots will be created
  save_plots: true          # Save plots to files
  show_plots: true          # Show plots
  plots_path: 'logml_plots' # Path where plots are saved

  # Max table size to show
  display_max_columns: 1000
  display.max_rows: 1000

#---
# Dataset parameters
#---
dataset:
  # Dataset type: 'df' means dataFrame
  dataset_type: 'df'

  # Dataset name:
  #   A simple name for the dataset.
  #   It is appended to dataset_path to create dataset_file_name
  dataset_name: 'my_dataset'

  # Dataset Path: Path to use when loading and saving datasets
  dataset_path: 'data/my_dataset'

  # If set, loading dataset from pickle file will be skipped
  do_not_load_pickle: false

  # If set, saving dataset will be skipped
  do_not_save: false

  # Use all inputs, no outputs.
  # I.e. do not split in/out.
  # E.g. unsupervised learning
  is_use_all_inputs: false

  # Use default 'in_out' method
  is_use_default_in_out: true

  # Use default preprocess
  is_use_default_preprocess: true

  # Use (internal) 'split' function if none is provided by the user?
  # Note: This function returns dataset as a list
  is_use_default_split: false

  # Use default transform operation
  is_use_default_transform: true

  # Output variables, a.k.a. dependent variables (i.e. what we want to predict)
  outputs: []

#---
# Dataset prerpocessing: Convert to categorical variable, dates, one hot,
# impute missing values, normalize, etc.
#---
dataset_preprocess:
  # Set to 'false' to disable this step
  enable: true

  # Balance an unbalanced datasets (classification models only, since outputs
  # must be cathegorical)
  balance: false

  # Categorical data
  # A list of categorical input variables
  # Can be followed by a list of categories to enforce some type of categorical
  # order in the codes
  categories:
    UsageBand: ['Low', 'Medium', 'High']
    ProductSize: ['Mini', 'Small', 'Compact', 'Medium', 'Large / Medium', 'Large']

  # Categorical data: Match field names using regular expressions
  # A list of categorical input variables
  # Can be followed by a list of categories to enforce some type of categorical
  # order in the codes
  categories_regex:
    - 'zzz_.*': ['low', 'mid', 'high']
    - 'yyy_.*': True

  # List of data columns that are transformed into 'date/time'
  # These columns are also split into 'date parts' (year, month, day,
  # day_of_week, etc.)
  dates: ['saledate']

  # Sanitize column names: Convert column names so that characters outside the
  # set [_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789] are
  # converted to '_' (underscore)
  is_sanitize_column_names: True

  # One hot encoding: List of variable to transform to one_hot
  ont_hot: ['Enclosure_Type']

  # One hot encoding: Encode all variables having a cardinality less or equal
  # to 'one_hot_max_cardinality'
  one_hot_max_cardinality: 7

  # Remove repeated inputs (i.e. input variables that have the exact same
  # values as other inputs)
  remove_equal_inputs: true

  # Remove rows having missing output/s
  remove_missing_outputs: true

  # Remove columns: List of columns to remove from dataFrame
  remove_columns: []

  # Remove columns: List of columns to remove from dataFrame (after all
  # transformations have been applied)
  remove_columns_after: []

  # Shuffle samples (e.g. shuffle rows in dataframe)
  shuffle: false

  # Drop inputs having standard deviation below this limit
  std_threshold: 0.0001

  # Impute values
  # Imputation method name, followed by list of variables, regex or 'true'
  # (this last option means "use as default method")
  # that all variables should be imputed using this method
  impute:
    # Use the mean value
    mean:  ['x2', 'x5', 'xx.*']

    # Impute using the median value
    median: true

    # Impute using the most frequent value
    most_frequent: ['x3', 'x4']

    # Impute by assigning value '1'
    one: ['x9', 'o.*']

    # Do not impute these variables
    skip: ['x7', 'x8']

    # Impute by assigning value '0'
    zero: ['x10', 'z.*']

  # Normalize values
  # Normalization methods followed by list of variables, regex or 'true' meaning
  # that all variables should be normalized using this method
  normalize:
    # Transform so that mean(x)=0 var(x)=1
    # Set to 'true' means to use as default (i.e. use this normalization
    # for all variables not explicitly define elsewhere)
    standard: true

    # Normalize dividing by max(abs(x_i))
    maxabs: ['x2', 'x5', 'xx.*']

    # Use min/max normalizer, i.e. transform to interval [0,1]
    minmax: ['x3']

    # Use min/max normalizer with negative values, i.e. transform to interval [-1,1]
    minmax_neg: ['x6', 'neg_.*']

    # Apply log(x)
    log: ['x9']

    # Apply log(x+1)
    log1p: ['x10', 'x11']

    # Apply log(x), then use standard transform: (log(x) - mean(log(x))) / std(log(x))
    log_standard: ['x9']

    # Apply log(x+1), then use standard transform: (log(x+1) - mean(log(x+1))) / std(log(x+1))
    log1p_standard: ['x10', 'x11']

    # Quantile transformation: This method transforms the features to follow a
    # uniform or a normal distribution
    quantile: ['q.*']

    # Do not normalize these variables
    skip: ['x7', 'x8']



#---
# Dataset augmentation: Add variable interaction terms
#---
dataset_augment:
  # Set to 'false' to disable this step
  enable: true
  # TODO: add_interactions: ['*', '-', '+', '/']

  # Add principal componenets
  # After the 'pca', you can add several "sub-sections":
  #   name:                 # Unique name, used to add PCA fields on each sample (name_1, name_2, ... name_num)
  #     num: 2              # Number of PCA componenets to add
  #     fields: ['x.*']     # List of regular expression to match field names (only fields matching this list will be used)
  pca:
    pca_dna:
      num: 2
      fields: ['DNA_.*']
    pca_log2:
      num: 3
      fields: ['log2ratio_.*', '.*_LOG2F']
  # Add NMF (non-negative matrix factorization)
  # After the 'nmf', you can add several "sub-sections":
  #   name:                 # Unique name, used to add NMF fields on each sample (name_1, name_2, ... name_num)
  #     num: 2              # Number of NMF componenets to add
  #     fields: ['x.*']     # List of regular expression to match field names (only fields matching this list will be used)
  nmf:
    nmf_dna:
      num: 2
      fields: ['DNA_.*']
    nmf_expr:
      num: 3
      fields: ['expr_.*', '.*_EXPR']

#---
# Dataset exploration
#---
dataset_explore:
  # Set to 'false' to disable this step
  enable: true

  # Perform correlation analysis
  is_correlation_analysis: true

  # Show dendogram
  is_dendogram: true

  # Describe all variables
  is_describe_all: true

  # Perform 'missing data' analysis
  is_nas: true

  # Plot pairs of variables
  is_plot_pairs: true

  # Create summary
  is_summary: true

  # Also explore 'original' dataset (i.e. before transforming)
  is_use_ori: false

  # Consider variables 'highly correlated' if over this threshold
  corr_thresdld: 0.7

  # Do not plot pairs if there are more than 'correlation_analysis_max' variables
  correlation_analysis_max: 100

  # When plotting a histogram, the columns should have at least this number of
  # unique values to show the 'kernel density estimate' line
  describe_kde_min_uniq_values: 100

  # Do not perform dendogram plot for more than 'dendogram_max' varaibles
  dendogram_max: 100

  # Do not plot pairs if there are more than 'plot_pairs_max' varaibles
  plot_pairs_max: 20

  # Consider a varaible 'normal' or 'log normal' if Shapiro-Wilks test is over this threshold
  shapiro_wilks_threshold: 0.1

#---
# Dataset: Feature importance / feature selection
#---
dataset_feature_importance:
  # Set to 'false' to disable this step
  enable: true

  # Set to 'false' to disable this step for 'na' dataset (i.e. a dataset of missing data)
  enable_na: true

  # Enable "Feature Importance using Permutations" (for different models)
  is_fip_random_forest: true
  is_fip_extra_trees: true
  is_fip_gradient_boosting: true

  # Regularization methods
  # Enable regularization methods (for different models)
  is_regularization_lasso: true
  is_regularization_ridge: true
  is_regularization_lars: true
  # Number of Cross-validation in regularization methods
  regularization_model_cv: 10

  # Enable Recursive Feature Elimination (for different models)
  is_rfe_model: true
  is_rfe_model_lasso: true
  is_rfe_model_ridge: true
  is_rfe_model_lars_aic: true
  is_rfe_model_lars_bic: true
  is_rfe_model_random_forest: true
  is_rfe_model_extra_trees: true
  is_rfe_model_gradient_boosting: true
  rfe_model_cv: 0  # Number of Corss-validations in Recursive Feature Elimination methods

  # Enable model selection methods (SelectFdr / SelectKBest)
  is_select: true

  # Linear regression p-value
  is_linear_pvalue: true
  # Variables used for setting the null model (always add to linear regression model)
  linear_pvalue_null_model_variables: ['x6']

  # Tree graph
  is_tree_graph: true
  tree_graph_max_depth: 4  # Number of layers to show in graph

  # Range to use when expanding weights
  # Note: Weights are converted to an interval [weight_min, weight_max]
  weight_max: 10.0
  weight_min: 1.0

  # Logistic regression p-value (Wilks)
  is_wilks: true
  # Variables used for setting the null model
  wilks_null_model_variables: ['age', 'sex', 'pc_1', 'pc_2', 'pc_3', 'pc_4']

#---
# Model definition and parameters
#---
model:
  # Set to 'false' to disable model learning (e.g. if we only want to perform data exploration)
  enable: true

  # Type of model: {'classification', 'regression', 'unsupervised'}
  model_type: regression

  # Model name: A simple string to use for file names related to this model
  model_name: 'my_model'

  # Buit-in model
  model_class: sklearn.ensemble.RandomForestRegressor

  # Train path: A path where to store logs and data from training
  model_path: 'data/my_model/model'

  # Save model using a pickle file?
  is_save_model_pickle: true

  # Save model using a 'model.save()'?
  is_save_model_method: true

  # Model file extesion
  is_save_model_method_ext: 'model'

  # Save parameters to YAML file
  is_save_params: true

  # Save model test results to a pickle file?
  is_save_test_pickle: true

  # Save model train results to pickle file?
  is_save_train_pickle: false

  # Save model validation results to pickle file?
  is_save_validate_pickle: false

  # Class (e.g. from sklearn.metric) to be used when evaluating the model
  metric_class: ''

  # Max value for a metric score, used to convert from 'score' to 'loss' (loss = metric_class_max - score)
  metric_class_max: ''

  # Is the metric a score? When no 'metric_class_max' is defined, convert by doing
  #     loss = -1 * score
  metric_class_is_score: False


#---
# Cross validation methodology
# Note: Cross validation and hyper parameter search cannot be enabled simultanously
# Note: These cross validation methods are from SkLearn
#---
cross_validation:
    # Set to 'false' to disable cross validation
    enable: false
    KFold:
        n_splits: 5
    # RepeatedKFold:
    #     n_splits: 5
    #     n_repeats: 2
    # LeaveOneOut:
    # LeavePOut:
    #     p: 2
    # ShuffleSplit:
    #     n_splits: 5
    #     test_size: 0.25
#---
# Hyper-parameter optimization.
#---
hyper_parameter_optimization:
    # Set this to 'true' to enable hyper-parameter optimization
    enable: false
    # Show progress bar
    show_progressbar: true
    # Algorithm:
    #   - 'tpe': Bayesian Tree of Parzen Estimators (this should be the default)
    #   - 'random': Random search
    algorithm: 'tpe'
    # Max number of hyper-parameter evaluations. Keep in mnd that each evaluation is a full model training
    max_evals: 100
    # Parameter space to explore
    space:
        # Parameters space for 'train_model'. Names have to match exactly the ones in 'train_model' parameters section
        # The format is:
        #     parameter_name: ['distribution', distribution)parameters...]
        # For distribution names and parametres, see:
        #     https://github.com/hyperopt/hyperopt/wiki/FMin
        #     (section 'Parameter Expressions')
        model_create:
            n_estimators: ['randint', 1000]
            min_samples_leaf: ['randint', 100]
            max_features: ['uniform', 0.0, 1.0]

#---
# User defined functions
# Parameters that are passed to the respective functions.
#
# E.g. If you have the parameter 'learn_reate = 0.01234' in 'train_model'
#      section, when your train function is invokde, it will be invoked as:
#           train_model(...., learn_reate=0.01234)
#      i.e. it will have kwargs={learn_reate:0.01234}
#      These parameters will also be saved to the training-specific YAML
#      file, so you can easily keep track of them
#
# Note: To disable individual items, you can add an 'enable: false' parameter
#---
functions:
    # User defined function: Dataset augmentation
    # Parameters: First parameter is always 'dataset'
    # Returns: Augmented dataset
    dataset_augment:

    # User defined function: Create dataset
    # Returns: A dataset
    dataset_create:

    # User defined function: Load dataset
    # Returns: A dataset
    dataset_load:

    # Split dataset's inputs and outputs
    # Returns a tuple (inputs, outputs)
    dataset_inout:

    # User defined function: Save dataset
    # Parameters: First four parameters are 'dataset, dataset_train, dataset_test, dataset_validate'
    dataset_save:

    # User defined function: Split dataset
    # Parameters: First parameter is always 'dataset'
    # Returns: Dataset split into three parts: 'dataset_train, dataset_test, dataset_validate'
    dataset_split:
      split_validate: 0.2
      split_test: 0.0

    # User defined function: Dataset pre-processing
    # Parameters: First parameter is always 'dataset'
    # Returns: Pre-processed dataset
    dataset_preprocess:

    # User defined function: Create model
    # Parameters: First parameter is 'dataset_train' (or 'dataset' if there is no dataset_test)
    # Returns: A model to be traind on dataset_train
    model_create:
      n_estimators: 30
      min_samples_leaf: 3
      max_features: 0.7

    # User defined function: Evaluate model
    # Parameters: First parameter is dataset to evaluate (usually dataset_test or dataset_validate)
    # Returns: Results from evaliating the dataset (usually a loss)
    model_evaluate:

    # User defined function: Save model
    # Parameters: First parameter is 'model'
    model_save:

    # User defined function: Training
    # Parameters: First parameters are 'model' and dataset (usually dataset_train)
    # Returns: Results from evaliating the dataset (usually a loss)
    model_train:

#---
# Model search
#---
model_search:
  # Set this to 'true' to enable model search
  enable: true

  # Include all models in 'model_search' directory
  models: !include models_search/*.yaml
