
#---
# Model search: Regression models
#---

#---
# Regression Models: Generalized linear regression
#---
# Linear regression
LinearRegression:
    model:
      model_type: regression
      model_class: sklearn.linear_model.LinearRegression
    functions:
      model_create:
        fit_intercept: True
        normalize: True
        copy_X: True
# Ridge regression (linear model + L2 norm penalty)
RidgeRegression:
    model:
      model_type: regression
      model_class: sklearn.linear_model.Ridge
    functions:
      model_create:
        alpha: 1.0
        fit_intercept: True
        normalize: True
        tol: 0.001
        solver: 'auto'
# Ridge regression with Cross-validation to search for alpha
RidgeCVRegression:
    model:
      model_type: regression
      model_class: sklearn.linear_model.RidgeCV
    functions:
      model_create:
        # alphas: [0.1, 1.0, 10.0]
        fit_intercept: True
        normalize: True
        store_cv_values: False
# Lasso regression (linear model + L1 norm penalty)
LassoRegression:
    model:
      model_type: regression
      model_class: sklearn.linear_model.Lasso
    functions:
      model_create:
        alpha: 1.0
        fit_intercept: True
        normalize: True
        precompute: False
        copy_X: True
        max_iter: 1000
        tol: 0.0001
        warm_start: False
        positive: False
        selection: 'cyclic'
# Lasso regression with alpha search using cross-validation
LassoCVRegression:
    model:
      model_type: regression
      model_class: sklearn.linear_model.LassoCV
    functions:
      model_create:
        eps: 0.001
        n_alphas: 100
        fit_intercept: True
        normalize: True
        precompute: 'auto'
        max_iter: 1000
        tol: 0.0001
        copy_X: True
        cv: 'warn'
        verbose: False
        positive: False
        selection: 'cyclic'
# ElasticNet: Linear regression model trained with both L1 and L2 norm regularization
# of the coefficients. Parameter search using Cross-validation
ElasticNetCV:
    model:
      model_type: regression
      model_class: sklearn.linear_model.OrthogonalMatchingPursuit
    functions:
      model_create:
        l1_ratio: 0.5
        eps: 0.001
        n_alphas: 100
        fit_intercept: True
        normalize: True
        precompute: 'auto'
        max_iter: 1000
        tol: 0.0001
        cv: 'warn'
        copy_X: True
        verbose: 0
        positive: False
        selection: 'cyclic'
# LARS regression (Least-Angle Regression)
LarsRegression:
    model:
      model_type: regression
      model_class: sklearn.linear_model.Lars
    functions:
      model_create:
        fit_intercept: True
        verbose: False
        normalize: True
        precompute: 'auto'
        n_nonzero_coefs: 500
        eps: 2.220446049250313e-16
        copy_X: True
        fit_path: True
# Orthogonal Matching Pursuit regression
OrthogonalMatchingPursuitRegression:
    model:
      model_type: regression
      model_class: sklearn.linear_model.OrthogonalMatchingPursuit
    functions:
      model_create:
        fit_intercept: True
        normalize: True
        precompute: 'auto'
# Bayesian Ridge regression
BayesianRidge:
    model:
      model_type: regression
      model_class: sklearn.linear_model.BayesianRidge
    functions:
      model_create:
        n_iter: 300
        tol: 0.001
        alpha_1: 1.0e-06
        alpha_2: 1.0e-06
        lambda_1: 1.0e-06
        lambda_2: 1.0e-06
        compute_score: False
        fit_intercept: True
        normalize: True
        copy_X: True
        verbose: False
# Automatic Relevance Determination: Similar to Bayesian Ridge Regression, but can lead to sparser coefficients
ARDRegression:
    model:
      model_type: regression
      model_class: sklearn.linear_model.ARDRegression
    functions:
      model_create:
        n_iter: 300
        tol: 0.001
        alpha_1: 1.0e-06
        alpha_2: 1.0e-06
        lambda_1: 1.0e-06
        lambda_2: 1.0e-06
        compute_score: False
        threshold_lambda: 10000.0
        fit_intercept: True
        normalize: True
        copy_X: True
        verbose: False
# RANSAC (RANdom SAmple Consensus) fits a model from random subsets of inliers from the complete data set
RANSACRegressor:
    model:
      model_type: regression
      model_class: sklearn.linear_model.RANSACRegressor
    functions:
      model_create:
        max_trials: 100
        max_skips: .inf
        stop_n_inliers: .inf
        stop_score: .inf
        stop_probability: 0.99
        loss: 'absolute_loss'
# TheilSenRegressor estimator uses a generalization of the median in multiple dimensions
TheilSenRegressor:
    model:
      model_type: regression
      model_class: sklearn.linear_model.TheilSenRegressor
    functions:
      model_create:
        fit_intercept: True
        copy_X: True
        max_subpopulation: 10000.0
        max_iter: 300
        tol: 0.001
        verbose: False
# HuberRegressor is "robust" Ridge because it applies a linear loss to samples that are classified as outliers
HuberRegressor:
    model:
      model_type: regression
      model_class: sklearn.linear_model.HuberRegressor
    functions:
      model_create:
        epsilon: 1.35
        max_iter: 100
        alpha: 0.0001
        warm_start: False
        fit_intercept: True
        tol: 1.0e-05
#---
# Regression Models: Support Vector Machines
#---
# Support Vector Machine Regressor
SVR:
    model:
      model_type: regression
      model_class: sklearn.svm.SVR
    functions:
      model_create:
        kernel: 'rbf'
        degree: 3
        gamma: 'auto_deprecated'
        coef0: 0.0
        tol: 0.001
        C: 1.0
        epsilon: 0.1
        shrinking: True
        cache_size: 200
        verbose: False
        max_iter: -1
# Support Vector Machine Regressor:
NuSVR:
    model:
      model_type: regression
      model_class: sklearn.svm.NuSVR
    functions:
      model_create:
        nu: 0.5
        C: 1.0
        kernel: 'rbf'
        degree: 3
        gamma: 'auto_deprecated'
        coef0: 0.0
        shrinking: True
        tol: 0.001
        cache_size: 200
        verbose: False
        max_iter: -1
# Support Vector Machine Regressor: Linear
LinearSVR:
    model:
      model_type: regression
      model_class: sklearn.svm.LinearSVR
    functions:
      model_create:
        epsilon: 0.0
        tol: 0.0001
        C: 1.0
        loss: 'epsilon_insensitive'
        fit_intercept: True
        intercept_scaling: 1.0
        dual: True
        verbose: 0
        # random_state: None
        max_iter: 1000
#---
# Regression Models: Nearest neighbors
#---
# Nearest neighbor: Regression based on k-nearest neighbors.
KNeighborsRegressor:
    model:
      model_type: regression
      model_class: sklearn.neighbors.KNeighborsRegressor
    functions:
      model_create:
        n_neighbors: 5
        weights: 'uniform'
        algorithm: 'auto'
        leaf_size: 30
        p: 2
        metric: 'minkowski'
        # metric_params: None
        # n_jobs: None
# Nearest neighbor: Regression based on fixed radius
RadiusNeighborsRegressor:
    enable: False   # WARING: Disabled. This model sometimes gives an error "Input contains NaN, infinity or a value too large for dtype('float64')"
    model:
      model_type: regression
      model_class: sklearn.neighbors.RadiusNeighborsRegressor
    functions:
      model_create:
        radius: 1.0
        weights: 'uniform'
        algorithm: 'auto'
        leaf_size: 30
        p: 2
        metric: 'minkowski'
        # metric_params: None
        # n_jobs: None
#---
# Regression Models: Ensemble
#---
# A decision tree regressor
DecisionTreeRegressor:
    model:
      model_type: classification
      model_class: sklearn.tree.DecisionTreeRegressor
    functions:
      model_create:
        criterion: 'mse'
        splitter: 'best'
        # max_depth: None
        min_samples_split: 2
        min_samples_leaf: 1
        min_weight_fraction_leaf: 0.0
        # max_features: None
        # random_state: None
        # max_leaf_nodes: None
        min_impurity_decrease: 0.0
        # min_impurity_split: None
        presort: False
# Random forest: An ensamble of random trees
RandomForestRegressor:
    model:
      model_type: regression
      model_class: sklearn.ensemble.RandomForestRegressor
    functions:
      model_create:
        n_estimators: 100
        criterion: 'mse'
        # max_depth: None
        min_samples_split: 2
        min_samples_leaf: 1
        min_weight_fraction_leaf: 0.0
        max_features: 'auto'
        # max_leaf_nodes: None
        min_impurity_decrease: 0.0
        # min_impurity_split: None
        bootstrap: True
        oob_score: False
        # n_jobs: None
        # random_state: None
        verbose: 0
        warm_start: False
    hyper_parameter_optimization:
      enable: False
      show_progressbar: True
      algorithm: 'tpe'
      max_evals: 100
      space:
          model_create:
              n_estimators: ['randint', 1000]
              min_samples_leaf: ['randint', 100]
              max_features: ['uniform', 0.3, 1.0]
